{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Part 1 -- Setup & Collection\n",
    "\n",
    "Pull Tweets from the Twitter API and collect **all Tweets from 30 tech thought leaders and news outlets**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Load lib codes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/predicting_stock_market_trends_with_Twitter/ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from os import chdir\n",
    "chdir('/home/jovyan/work/predicting_stock_market_trends_with_Twitter/')\n",
    "\n",
    "from lib import *\n",
    "# suppress_warnings()\n",
    "from lib.twitter_keys import my_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymongo in /opt/conda/envs/python2/lib/python2.7/site-packages\n",
      "Requirement already satisfied: pyquery in /opt/conda/envs/python2/lib/python2.7/site-packages\n",
      "Requirement already satisfied: tweepy in /opt/conda/envs/python2/lib/python2.7/site-packages\n",
      "Requirement already satisfied: lxml>=2.1 in /opt/conda/envs/python2/lib/python2.7/site-packages (from pyquery)\n",
      "Requirement already satisfied: cssselect>0.7.9 in /opt/conda/envs/python2/lib/python2.7/site-packages (from pyquery)\n",
      "Requirement already satisfied: requests>=2.4.3 in /opt/conda/envs/python2/lib/python2.7/site-packages (from tweepy)\n",
      "Requirement already satisfied: requests-oauthlib>=0.4.1 in /opt/conda/envs/python2/lib/python2.7/site-packages (from tweepy)\n",
      "Requirement already satisfied: six>=1.7.3 in /opt/conda/envs/python2/lib/python2.7/site-packages (from tweepy)\n",
      "Requirement already satisfied: oauthlib>=0.6.2 in /opt/conda/envs/python2/lib/python2.7/site-packages (from requests-oauthlib>=0.4.1->tweepy)\n"
     ]
    }
   ],
   "source": [
    "!pip install pymongo pyquery tweepy\n",
    "import pymongo\n",
    "import tweepy\n",
    "# from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Replace the API_KEY and API_SECRET with your application's key and secret\n",
    "auth = tweepy.AppAuthHandler(my_keys['CONSUMER_KEY'], my_keys['CONSUMER_SECRET'])\n",
    "\n",
    "# Authorize twitter, initialize tweepy\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "\n",
    "if (not api):\n",
    "    print (\"Can't Authenticate\")\n",
    "    sys.exit(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cli = pymongo.MongoClient(host='35.163.253.8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'twitter_db']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiates when you put data in \n",
    "status_collection = cli.twitter_db.status_collection\n",
    "complete_collection = cli.twitter_db.complete_collection\n",
    "test_collection = cli.twitter_db.test_collection\n",
    "tweets_collection = cli.twitter_db.tweets_collection\n",
    "cli.database_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(status_collection.count())\n",
    "print(complete_collection.count())\n",
    "print(incomplete_collection.count())\n",
    "print(tweets_collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "status_collection.drop()\n",
    "complete_collection.drop()\n",
    "test_collection.drop()\n",
    "tweets_collection.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lxml==3.5.0 (from -r requirements.txt (line 1))\n",
      "  Using cached lxml-3.5.0.tar.gz\n",
      "Collecting pyquery==1.2.10 (from -r requirements.txt (line 2))\n",
      "  Using cached pyquery-1.2.10-py2-none-any.whl\n",
      "Requirement already satisfied: cssselect>0.7.9 in /opt/conda/envs/python2/lib/python2.7/site-packages (from pyquery==1.2.10->-r requirements.txt (line 2))\n",
      "Building wheels for collected packages: lxml\n",
      "  Running setup.py bdist_wheel for lxml ... \u001b[?25l-\b \b\\\b \b|\b \berror\n",
      "  Complete output from command /opt/conda/envs/python2/bin/python -u -c \"import setuptools, tokenize;__file__='/tmp/pip-build-woXIgI/lxml/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" bdist_wheel -d /tmp/tmpgVpaoypip-wheel- --python-tag cp27:\n",
      "  Building lxml version 3.5.0.\n",
      "  Building without Cython.\n",
      "  ERROR: /bin/sh: 1: xslt-config: not found\n",
      "  \n",
      "  ** make sure the development packages of libxml2 and libxslt are installed **\n",
      "  \n",
      "  Using build configuration of libxslt\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build/lib.linux-x86_64-2.7\n",
      "  creating build/lib.linux-x86_64-2.7/lxml\n",
      "  copying src/lxml/__init__.py -> build/lib.linux-x86_64-2.7/lxml\n",
      "  copying src/lxml/usedoctest.py -> build/lib.linux-x86_64-2.7/lxml\n",
      "  copying src/lxml/pyclasslookup.py -> build/lib.linux-x86_64-2.7/lxml\n",
      "  copying src/lxml/_elementpath.py -> build/lib.linux-x86_64-2.7/lxml\n",
      "  copying src/lxml/cssselect.py -> build/lib.linux-x86_64-2.7/lxml\n",
      "  copying src/lxml/builder.py -> build/lib.linux-x86_64-2.7/lxml\n",
      "  copying src/lxml/sax.py -> build/lib.linux-x86_64-2.7/lxml\n",
      "  copying src/lxml/doctestcompare.py -> build/lib.linux-x86_64-2.7/lxml\n",
      "  copying src/lxml/ElementInclude.py -> build/lib.linux-x86_64-2.7/lxml\n",
      "  creating build/lib.linux-x86_64-2.7/lxml/includes\n",
      "  copying src/lxml/includes/__init__.py -> build/lib.linux-x86_64-2.7/lxml/includes\n",
      "  creating build/lib.linux-x86_64-2.7/lxml/html\n",
      "  copying src/lxml/html/soupparser.py -> build/lib.linux-x86_64-2.7/lxml/html\n",
      "  copying src/lxml/html/_html5builder.py -> build/lib.linux-x86_64-2.7/lxml/html\n",
      "  copying src/lxml/html/formfill.py -> build/lib.linux-x86_64-2.7/lxml/html\n",
      "  copying src/lxml/html/__init__.py -> build/lib.linux-x86_64-2.7/lxml/html\n",
      "  copying src/lxml/html/usedoctest.py -> build/lib.linux-x86_64-2.7/lxml/html\n",
      "  copying src/lxml/html/ElementSoup.py -> build/lib.linux-x86_64-2.7/lxml/html\n",
      "  copying src/lxml/html/html5parser.py -> build/lib.linux-x86_64-2.7/lxml/html\n",
      "  copying src/lxml/html/_setmixin.py -> build/lib.linux-x86_64-2.7/lxml/html\n",
      "  copying src/lxml/html/builder.py -> build/lib.linux-x86_64-2.7/lxml/html\n",
      "  copying src/lxml/html/defs.py -> build/lib.linux-x86_64-2.7/lxml/html\n",
      "  copying src/lxml/html/_diffcommand.py -> build/lib.linux-x86_64-2.7/lxml/html\n",
      "  copying src/lxml/html/clean.py -> build/lib.linux-x86_64-2.7/lxml/html\n",
      "  copying src/lxml/html/diff.py -> build/lib.linux-x86_64-2.7/lxml/html\n",
      "  creating build/lib.linux-x86_64-2.7/lxml/isoschematron\n",
      "  copying src/lxml/isoschematron/__init__.py -> build/lib.linux-x86_64-2.7/lxml/isoschematron\n",
      "  copying src/lxml/lxml.etree.h -> build/lib.linux-x86_64-2.7/lxml\n",
      "  copying src/lxml/lxml.etree_api.h -> build/lib.linux-x86_64-2.7/lxml\n",
      "  copying src/lxml/includes/config.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n",
      "  copying src/lxml/includes/xmlerror.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n",
      "  copying src/lxml/includes/tree.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n",
      "  copying src/lxml/includes/schematron.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n",
      "  copying src/lxml/includes/htmlparser.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n",
      "  copying src/lxml/includes/xmlparser.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n",
      "  copying src/lxml/includes/xslt.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n",
      "  copying src/lxml/includes/xinclude.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n",
      "  copying src/lxml/includes/uri.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n",
      "  copying src/lxml/includes/dtdvalid.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n",
      "  copying src/lxml/includes/xpath.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n",
      "  copying src/lxml/includes/xmlschema.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n",
      "  copying src/lxml/includes/relaxng.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n",
      "  copying src/lxml/includes/etreepublic.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n",
      "  copying src/lxml/includes/c14n.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n",
      "  copying src/lxml/includes/etree_defs.h -> build/lib.linux-x86_64-2.7/lxml/includes\n",
      "  copying src/lxml/includes/lxml-version.h -> build/lib.linux-x86_64-2.7/lxml/includes\n",
      "  creating build/lib.linux-x86_64-2.7/lxml/isoschematron/resources\n",
      "  creating build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/rng\n",
      "  copying src/lxml/isoschematron/resources/rng/iso-schematron.rng -> build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/rng\n",
      "  creating build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/xsl\n",
      "  copying src/lxml/isoschematron/resources/xsl/XSD2Schtrn.xsl -> build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/xsl\n",
      "  copying src/lxml/isoschematron/resources/xsl/RNG2Schtrn.xsl -> build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/xsl\n",
      "  creating build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/xsl/iso-schematron-xslt1\n",
      "  copying src/lxml/isoschematron/resources/xsl/iso-schematron-xslt1/iso_schematron_message.xsl -> build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/xsl/iso-schematron-xslt1\n",
      "  copying src/lxml/isoschematron/resources/xsl/iso-schematron-xslt1/iso_schematron_skeleton_for_xslt1.xsl -> build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/xsl/iso-schematron-xslt1\n",
      "  copying src/lxml/isoschematron/resources/xsl/iso-schematron-xslt1/iso_dsdl_include.xsl -> build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/xsl/iso-schematron-xslt1\n",
      "  copying src/lxml/isoschematron/resources/xsl/iso-schematron-xslt1/iso_svrl_for_xslt1.xsl -> build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/xsl/iso-schematron-xslt1\n",
      "  copying src/lxml/isoschematron/resources/xsl/iso-schematron-xslt1/iso_abstract_expand.xsl -> build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/xsl/iso-schematron-xslt1\n",
      "  copying src/lxml/isoschematron/resources/xsl/iso-schematron-xslt1/readme.txt -> build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/xsl/iso-schematron-xslt1\n",
      "  running build_ext\n",
      "  building 'lxml.etree' extension\n",
      "  creating build/temp.linux-x86_64-2.7\n",
      "  creating build/temp.linux-x86_64-2.7/src\n",
      "  creating build/temp.linux-x86_64-2.7/src/lxml\n",
      "  gcc -pthread -fno-strict-aliasing -g -O2 -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -Isrc/lxml/includes -I/opt/conda/envs/python2/include/python2.7 -c src/lxml/lxml.etree.c -o build/temp.linux-x86_64-2.7/src/lxml/lxml.etree.o -w\n",
      "  In file included from src/lxml/lxml.etree.c:323:0:\n",
      "  src/lxml/includes/etree_defs.h:14:31: fatal error: libxml/xmlversion.h: No such file or directory\n",
      "   #include \"libxml/xmlversion.h\"\n",
      "                                 ^\n",
      "  compilation terminated.\n",
      "  Compile failed: command 'gcc' failed with exit status 1\n",
      "  creating tmp\n",
      "  cc -I/usr/include/libxml2 -c /tmp/xmlXPathInitOfKJxW.c -o tmp/xmlXPathInitOfKJxW.o\n",
      "  /tmp/xmlXPathInitOfKJxW.c:1:26: fatal error: libxml/xpath.h: No such file or directory\n",
      "   #include \"libxml/xpath.h\"\n",
      "                            ^\n",
      "  compilation terminated.\n",
      "  *********************************************************************************\n",
      "  Could not find function xmlCheckVersion in library libxml2. Is libxml2 installed?\n",
      "  *********************************************************************************\n",
      "  error: command 'gcc' failed with exit status 1\n",
      "  \n",
      "  ----------------------------------------\n",
      "\u001b[31m  Failed building wheel for lxml\u001b[0m\n",
      "\u001b[?25h  Running setup.py clean for lxml\n",
      "Failed to build lxml\n",
      "Installing collected packages: lxml, pyquery\n",
      "  Found existing installation: lxml 3.7.3\n",
      "    Uninstalling lxml-3.7.3:\n",
      "      Successfully uninstalled lxml-3.7.3\n",
      "  Running setup.py install for lxml ... \u001b[?25l-\b \b\\\b \b|\b \berror\n",
      "    Complete output from command /opt/conda/envs/python2/bin/python -u -c \"import setuptools, tokenize;__file__='/tmp/pip-build-woXIgI/lxml/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" install --record /tmp/pip-VRP9v7-record/install-record.txt --single-version-externally-managed --compile:\n",
      "    Building lxml version 3.5.0.\n",
      "    Building without Cython.\n",
      "    ERROR: /bin/sh: 1: xslt-config: not found\n",
      "    \n",
      "    ** make sure the development packages of libxml2 and libxslt are installed **\n",
      "    \n",
      "    Using build configuration of libxslt\n",
      "    running install\n",
      "    running build\n",
      "    running build_py\n",
      "    creating build\n",
      "    creating build/lib.linux-x86_64-2.7\n",
      "    creating build/lib.linux-x86_64-2.7/lxml\n",
      "    copying src/lxml/__init__.py -> build/lib.linux-x86_64-2.7/lxml\n",
      "    copying src/lxml/usedoctest.py -> build/lib.linux-x86_64-2.7/lxml\n",
      "    copying src/lxml/pyclasslookup.py -> build/lib.linux-x86_64-2.7/lxml\n",
      "    copying src/lxml/_elementpath.py -> build/lib.linux-x86_64-2.7/lxml\n",
      "    copying src/lxml/cssselect.py -> build/lib.linux-x86_64-2.7/lxml\n",
      "    copying src/lxml/builder.py -> build/lib.linux-x86_64-2.7/lxml\n",
      "    copying src/lxml/sax.py -> build/lib.linux-x86_64-2.7/lxml\n",
      "    copying src/lxml/doctestcompare.py -> build/lib.linux-x86_64-2.7/lxml\n",
      "    copying src/lxml/ElementInclude.py -> build/lib.linux-x86_64-2.7/lxml\n",
      "    creating build/lib.linux-x86_64-2.7/lxml/includes\n",
      "    copying src/lxml/includes/__init__.py -> build/lib.linux-x86_64-2.7/lxml/includes\n",
      "    creating build/lib.linux-x86_64-2.7/lxml/html\n",
      "    copying src/lxml/html/soupparser.py -> build/lib.linux-x86_64-2.7/lxml/html\n",
      "    copying src/lxml/html/_html5builder.py -> build/lib.linux-x86_64-2.7/lxml/html\n",
      "    copying src/lxml/html/formfill.py -> build/lib.linux-x86_64-2.7/lxml/html\n",
      "    copying src/lxml/html/__init__.py -> build/lib.linux-x86_64-2.7/lxml/html\n",
      "    copying src/lxml/html/usedoctest.py -> build/lib.linux-x86_64-2.7/lxml/html\n",
      "    copying src/lxml/html/ElementSoup.py -> build/lib.linux-x86_64-2.7/lxml/html\n",
      "    copying src/lxml/html/html5parser.py -> build/lib.linux-x86_64-2.7/lxml/html\n",
      "    copying src/lxml/html/_setmixin.py -> build/lib.linux-x86_64-2.7/lxml/html\n",
      "    copying src/lxml/html/builder.py -> build/lib.linux-x86_64-2.7/lxml/html\n",
      "    copying src/lxml/html/defs.py -> build/lib.linux-x86_64-2.7/lxml/html\n",
      "    copying src/lxml/html/_diffcommand.py -> build/lib.linux-x86_64-2.7/lxml/html\n",
      "    copying src/lxml/html/clean.py -> build/lib.linux-x86_64-2.7/lxml/html\n",
      "    copying src/lxml/html/diff.py -> build/lib.linux-x86_64-2.7/lxml/html\n",
      "    creating build/lib.linux-x86_64-2.7/lxml/isoschematron\n",
      "    copying src/lxml/isoschematron/__init__.py -> build/lib.linux-x86_64-2.7/lxml/isoschematron\n",
      "    copying src/lxml/lxml.etree.h -> build/lib.linux-x86_64-2.7/lxml\n",
      "    copying src/lxml/lxml.etree_api.h -> build/lib.linux-x86_64-2.7/lxml\n",
      "    copying src/lxml/includes/config.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n",
      "    copying src/lxml/includes/xmlerror.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n",
      "    copying src/lxml/includes/tree.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n",
      "    copying src/lxml/includes/schematron.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n",
      "    copying src/lxml/includes/htmlparser.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n",
      "    copying src/lxml/includes/xmlparser.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n",
      "    copying src/lxml/includes/xslt.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n",
      "    copying src/lxml/includes/xinclude.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n",
      "    copying src/lxml/includes/uri.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n",
      "    copying src/lxml/includes/dtdvalid.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n",
      "    copying src/lxml/includes/xpath.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n",
      "    copying src/lxml/includes/xmlschema.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n",
      "    copying src/lxml/includes/relaxng.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n",
      "    copying src/lxml/includes/etreepublic.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n",
      "    copying src/lxml/includes/c14n.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n",
      "    copying src/lxml/includes/etree_defs.h -> build/lib.linux-x86_64-2.7/lxml/includes\n",
      "    copying src/lxml/includes/lxml-version.h -> build/lib.linux-x86_64-2.7/lxml/includes\n",
      "    creating build/lib.linux-x86_64-2.7/lxml/isoschematron/resources\n",
      "    creating build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/rng\n",
      "    copying src/lxml/isoschematron/resources/rng/iso-schematron.rng -> build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/rng\n",
      "    creating build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/xsl\n",
      "    copying src/lxml/isoschematron/resources/xsl/XSD2Schtrn.xsl -> build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/xsl\n",
      "    copying src/lxml/isoschematron/resources/xsl/RNG2Schtrn.xsl -> build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/xsl\n",
      "    creating build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/xsl/iso-schematron-xslt1\n",
      "    copying src/lxml/isoschematron/resources/xsl/iso-schematron-xslt1/iso_schematron_message.xsl -> build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/xsl/iso-schematron-xslt1\n",
      "    copying src/lxml/isoschematron/resources/xsl/iso-schematron-xslt1/iso_schematron_skeleton_for_xslt1.xsl -> build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/xsl/iso-schematron-xslt1\n",
      "    copying src/lxml/isoschematron/resources/xsl/iso-schematron-xslt1/iso_dsdl_include.xsl -> build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/xsl/iso-schematron-xslt1\n",
      "    copying src/lxml/isoschematron/resources/xsl/iso-schematron-xslt1/iso_svrl_for_xslt1.xsl -> build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/xsl/iso-schematron-xslt1\n",
      "    copying src/lxml/isoschematron/resources/xsl/iso-schematron-xslt1/iso_abstract_expand.xsl -> build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/xsl/iso-schematron-xslt1\n",
      "    copying src/lxml/isoschematron/resources/xsl/iso-schematron-xslt1/readme.txt -> build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/xsl/iso-schematron-xslt1\n",
      "    running build_ext\n",
      "    building 'lxml.etree' extension\n",
      "    creating build/temp.linux-x86_64-2.7\n",
      "    creating build/temp.linux-x86_64-2.7/src\n",
      "    creating build/temp.linux-x86_64-2.7/src/lxml\n",
      "    gcc -pthread -fno-strict-aliasing -g -O2 -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -Isrc/lxml/includes -I/opt/conda/envs/python2/include/python2.7 -c src/lxml/lxml.etree.c -o build/temp.linux-x86_64-2.7/src/lxml/lxml.etree.o -w\n",
      "    In file included from src/lxml/lxml.etree.c:323:0:\n",
      "    src/lxml/includes/etree_defs.h:14:31: fatal error: libxml/xmlversion.h: No such file or directory\n",
      "     #include \"libxml/xmlversion.h\"\n",
      "                                   ^\n",
      "    compilation terminated.\n",
      "    Compile failed: command 'gcc' failed with exit status 1\n",
      "    cc -I/usr/include/libxml2 -c /tmp/xmlXPathInit9OH7om.c -o tmp/xmlXPathInit9OH7om.o\n",
      "    /tmp/xmlXPathInit9OH7om.c:1:26: fatal error: libxml/xpath.h: No such file or directory\n",
      "     #include \"libxml/xpath.h\"\n",
      "                              ^\n",
      "    compilation terminated.\n",
      "    *********************************************************************************\n",
      "    Could not find function xmlCheckVersion in library libxml2. Is libxml2 installed?\n",
      "    *********************************************************************************\n",
      "    error: command 'gcc' failed with exit status 1\n",
      "    \n",
      "    ----------------------------------------\n",
      "\u001b[?25h  Rolling back uninstall of lxml\n",
      "\u001b[31mCommand \"/opt/conda/envs/python2/bin/python -u -c \"import setuptools, tokenize;__file__='/tmp/pip-build-woXIgI/lxml/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" install --record /tmp/pip-VRP9v7-record/install-record.txt --single-version-externally-managed --compile\" failed with error code 1 in /tmp/pip-build-woXIgI/lxml/\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "chdir('//home/jovyan/work/predicting_stock_market_trends_with_Twitter/tools/GetOldTweets-python-master')\n",
    "!pip install -r requirements.txt\n",
    "import got"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Implement distributed processing from scratch - run this on multiple AWS instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Handle</th>\n",
       "      <th>Followers</th>\n",
       "      <th>Description</th>\n",
       "      <th>Type</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ashton Kutcher</td>\n",
       "      <td>aplusk</td>\n",
       "      <td>18,100,000</td>\n",
       "      <td>Celebrity</td>\n",
       "      <td>Celebrity</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Mark Lehman</td>\n",
       "      <td>markflowchatter</td>\n",
       "      <td>33,400</td>\n",
       "      <td>has more than 23 years of experience in the ma...</td>\n",
       "      <td>Thought Leader</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MarketWatch</td>\n",
       "      <td>MarketWatch</td>\n",
       "      <td>3,390,000</td>\n",
       "      <td>News, personal finance &amp; commentary from Marke...</td>\n",
       "      <td>News Outlet</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Name           Handle    Followers   \\\n",
       "5   Ashton Kutcher           aplusk   18,100,000    \n",
       "26     Mark Lehman  markflowchatter       33,400    \n",
       "12     MarketWatch      MarketWatch    3,390,000    \n",
       "\n",
       "                                          Description            Type  \\\n",
       "5                                           Celebrity       Celebrity   \n",
       "26  has more than 23 years of experience in the ma...  Thought Leader   \n",
       "12  News, personal finance & commentary from Marke...     News Outlet   \n",
       "\n",
       "    Unnamed: 5  Unnamed: 6  Unnamed: 7  Unnamed: 8  \n",
       "5          NaN         NaN         NaN         NaN  \n",
       "26         NaN         NaN         NaN         NaN  \n",
       "12         NaN         NaN         NaN         NaN  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_30_df = pd.read_csv('/home/jovyan/work/predicting_stock_market_trends_with_Twitter/data/twitter_users_30.csv')\n",
    "twitter_30_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "handle = twitter_30_df['Handle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# for i, h in enumerate(handle):\n",
    "#     status_collection.insert_one({'handle': h})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_tweets_to_mongo(lookup):\n",
    "\n",
    "    maxTweets = 10000000 # Some arbitrary large number\n",
    "    tweetsPerQry = 200  # this is the max the API permits\n",
    "\n",
    "    for l in lookup:\n",
    "        # If results from a specific ID onwards are reqd, set since_id to that ID.\n",
    "        # else default to no lower limit, go as far back as API allows\n",
    "        sinceId = None\n",
    "\n",
    "        # If results only below a specific ID are, set max_id to that ID.\n",
    "        # else default to no upper limit, start from the most recent tweet matching the search query.\n",
    "        max_id = -1E10\n",
    "\n",
    "        tweetCount = 0\n",
    "        while tweetCount < maxTweets:\n",
    "            try:\n",
    "                if (max_id <= 0):\n",
    "                    if (not sinceId):\n",
    "                        new_tweets = api.user_timeline(screen_name = 'aplusk', count=tweetsPerQry)\n",
    "                    else:\n",
    "                        new_tweets = api.user_timeline(screen_name = 'aplusk', count=tweetsPerQry,\n",
    "                                                    since_id=sinceId)\n",
    "                else:\n",
    "\n",
    "                    if (not sinceId):\n",
    "                        new_tweets = api.user_timeline(screen_name = 'aplusk', count=tweetsPerQry,\n",
    "                                                    max_id=str(max_id - 1))\n",
    "                    else:\n",
    "                        new_tweets = api.user_timeline(screen_name = 'aplusk', count=tweetsPerQry,\n",
    "                                                    max_id=str(max_id - 1),\n",
    "                                                    since_id=sinceId)\n",
    "                if not new_tweets:\n",
    "                    print(\"No more tweets found\")\n",
    "                    break\n",
    "                for tweet in new_tweets:\n",
    "                    tweet_dict = {'username': tweet.user.screen_name,\n",
    "                                  'timestamp': tweet.created_at, \n",
    "                                  'text': tweet.text.encode(\"utf-8\")}\n",
    "        #             writer = csv.writer(f)\n",
    "        #             writer.writerow([\"id\",\"created_at\",\"text\"])\n",
    "        #             writer.writerows(outtweets)\n",
    "                    mongo_response = tweets_collection.insert_one(tweet_dict)\n",
    "\n",
    "                tweetCount += len(new_tweets)\n",
    "                print(\"Downloaded {0} tweets for user: {1}\".format(tweetCount, l))\n",
    "                max_id = new_tweets[-1].id  \n",
    "\n",
    "            except tweepy.TweepError as e:\n",
    "                # Just exit if any error\n",
    "                print(\"some error : \" + str(e))\n",
    "                break\n",
    "\n",
    "        print (\"Downloaded {0} tweets for user: {1} & saved to Mongo\\n\".format(tweetCount, l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 200 tweets for user: BarackObama\n",
      "Downloaded 400 tweets for user: BarackObama\n",
      "Downloaded 600 tweets for user: BarackObama\n",
      "Downloaded 799 tweets for user: BarackObama\n",
      "Downloaded 999 tweets for user: BarackObama\n",
      "Downloaded 1199 tweets for user: BarackObama\n",
      "Downloaded 1399 tweets for user: BarackObama\n",
      "Downloaded 1599 tweets for user: BarackObama\n",
      "Downloaded 1799 tweets for user: BarackObama\n",
      "Downloaded 1999 tweets for user: BarackObama\n",
      "Downloaded 2198 tweets for user: BarackObama\n",
      "Downloaded 2398 tweets for user: BarackObama\n",
      "Downloaded 2598 tweets for user: BarackObama\n",
      "Downloaded 2798 tweets for user: BarackObama\n",
      "Downloaded 2998 tweets for user: BarackObama\n",
      "Downloaded 3198 tweets for user: BarackObama\n",
      "Downloaded 3199 tweets for user: BarackObama\n",
      "No more tweets found\n",
      "\n",
      "Downloaded 3199 tweets for user: BarackObama & saved to Mongo\n",
      "\n",
      "\n",
      "Downloaded 200 tweets for user: cnnbrk\n",
      "Downloaded 400 tweets for user: cnnbrk\n",
      "Downloaded 600 tweets for user: cnnbrk\n",
      "Downloaded 799 tweets for user: cnnbrk\n",
      "Downloaded 999 tweets for user: cnnbrk\n",
      "Downloaded 1199 tweets for user: cnnbrk\n",
      "Downloaded 1399 tweets for user: cnnbrk\n",
      "Downloaded 1599 tweets for user: cnnbrk\n",
      "Downloaded 1799 tweets for user: cnnbrk\n",
      "Downloaded 1999 tweets for user: cnnbrk\n",
      "Downloaded 2198 tweets for user: cnnbrk\n",
      "Downloaded 2398 tweets for user: cnnbrk\n",
      "Downloaded 2598 tweets for user: cnnbrk\n",
      "Downloaded 2798 tweets for user: cnnbrk\n",
      "Downloaded 2998 tweets for user: cnnbrk\n",
      "Downloaded 3198 tweets for user: cnnbrk\n",
      "Downloaded 3199 tweets for user: cnnbrk\n",
      "No more tweets found\n",
      "\n",
      "Downloaded 3199 tweets for user: cnnbrk & saved to Mongo\n",
      "\n",
      "\n",
      "Downloaded 200 tweets for user: BillGates\n",
      "Downloaded 400 tweets for user: BillGates\n",
      "Downloaded 600 tweets for user: BillGates\n",
      "Downloaded 799 tweets for user: BillGates\n",
      "Downloaded 999 tweets for user: BillGates\n",
      "Downloaded 1199 tweets for user: BillGates\n",
      "Downloaded 1399 tweets for user: BillGates\n",
      "Downloaded 1599 tweets for user: BillGates\n",
      "Downloaded 1799 tweets for user: BillGates\n",
      "Downloaded 1999 tweets for user: BillGates\n",
      "Downloaded 2198 tweets for user: BillGates\n",
      "Downloaded 2398 tweets for user: BillGates\n",
      "Downloaded 2598 tweets for user: BillGates\n",
      "Downloaded 2798 tweets for user: BillGates\n",
      "Downloaded 2998 tweets for user: BillGates\n",
      "Downloaded 3198 tweets for user: BillGates\n",
      "Downloaded 3199 tweets for user: BillGates\n",
      "No more tweets found\n",
      "\n",
      "Downloaded 3199 tweets for user: BillGates & saved to Mongo\n",
      "\n",
      "\n",
      "Downloaded 200 tweets for user: realDonaldTrump\n",
      "Downloaded 400 tweets for user: realDonaldTrump\n",
      "Downloaded 600 tweets for user: realDonaldTrump\n",
      "Downloaded 799 tweets for user: realDonaldTrump\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-dcbeff024ee8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mget_tweets_to_mongo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-663784317f26>\u001b[0m in \u001b[0;36mget_tweets_to_mongo\u001b[0;34m(lookup)\u001b[0m\n\u001b[1;32m     26\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0msinceId\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                         new_tweets = api.user_timeline(screen_name = 'aplusk', count=tweetsPerQry,\n\u001b[0;32m---> 28\u001b[0;31m                                                     max_id=str(max_id - 1))\n\u001b[0m\u001b[1;32m     29\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                         new_tweets = api.user_timeline(screen_name = 'aplusk', count=tweetsPerQry,\n",
      "\u001b[0;32m/opt/conda/envs/python2/lib/python2.7/site-packages/tweepy/binder.pyc\u001b[0m in \u001b[0;36m_call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;31m# Set pagination mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python2/lib/python2.7/site-packages/tweepy/binder.pyc\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    185\u001b[0m                                                 \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m                                                 \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m                                                 proxies=self.api.proxy)\n\u001b[0m\u001b[1;32m    188\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mTweepError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Failed to send request: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python2/lib/python2.7/site-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    486\u001b[0m         }\n\u001b[1;32m    487\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python2/lib/python2.7/site-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m             \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python2/lib/python2.7/site-packages/requests/models.pyc\u001b[0m in \u001b[0;36mcontent\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    795\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCONTENT_CHUNK_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_content_consumed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python2/lib/python2.7/site-packages/requests/models.pyc\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'stream'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mProtocolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python2/lib/python2.7/site-packages/requests/packages/urllib3/response.pyc\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_fp_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python2/lib/python2.7/site-packages/requests/packages/urllib3/response.pyc\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0mcache_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Platform-specific: Buggy versions of Python.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m                     \u001b[0;31m# Close the connection when no data is returned\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python2/lib/python2.7/httplib.pyc\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    610\u001b[0m         \u001b[0;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mamt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python2/lib/python2.7/socket.pyc\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    382\u001b[0m                 \u001b[0;31m# fragmentation issues on many platforms.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mEINTR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python2/lib/python2.7/ssl.pyc\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m    754\u001b[0m                     \u001b[0;34m\"non-zero flags not allowed in calls to recv() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m                     self.__class__)\n\u001b[0;32m--> 756\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    757\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python2/lib/python2.7/ssl.pyc\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    641\u001b[0m                 \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m                 \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "start = datetime.now()\n",
    "get_tweets_to_mongo(handle)\n",
    "end = datetime.now()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3199"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>text</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>username</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5931c544307e01029e6614e9</td>\n",
       "      <td>RT @weatherchannel: BREAKING: Trump pulls U.S....</td>\n",
       "      <td>2017-06-02 00:28:21</td>\n",
       "      <td>aplusk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5931c544307e01029e6614ea</td>\n",
       "      <td>RT @elonmusk: Under Paris deal, China committe...</td>\n",
       "      <td>2017-06-02 00:12:33</td>\n",
       "      <td>aplusk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5931c544307e01029e6614eb</td>\n",
       "      <td>RT @ddiamond: Countries that support Paris cli...</td>\n",
       "      <td>2017-06-02 00:11:58</td>\n",
       "      <td>aplusk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5931c544307e01029e6614ec</td>\n",
       "      <td>@Billiegaga @realDonaldTrump Having a sense of...</td>\n",
       "      <td>2017-06-01 01:27:32</td>\n",
       "      <td>aplusk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5931c544307e01029e6614ed</td>\n",
       "      <td>There has to be a new strain of medical mariju...</td>\n",
       "      <td>2017-06-01 01:23:28</td>\n",
       "      <td>aplusk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5931c544307e01029e6614ee</td>\n",
       "      <td>Getting ready to hit the old #Covfefe</td>\n",
       "      <td>2017-06-01 01:14:39</td>\n",
       "      <td>aplusk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5931c544307e01029e6614ef</td>\n",
       "      <td>@garthbrooks @Favre4Official @serenawilliams @...</td>\n",
       "      <td>2017-05-30 20:54:28</td>\n",
       "      <td>aplusk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5931c544307e01029e6614f0</td>\n",
       "      <td>@haibon_jared @thorn We are happy to have your...</td>\n",
       "      <td>2017-05-30 20:49:56</td>\n",
       "      <td>aplusk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5931c544307e01029e6614f1</td>\n",
       "      <td>Ready to help @TheRachLindsay make some hard d...</td>\n",
       "      <td>2017-05-29 18:52:48</td>\n",
       "      <td>aplusk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5931c544307e01029e6614f2</td>\n",
       "      <td>The right to pursue happiness belong to everyo...</td>\n",
       "      <td>2017-05-29 13:14:04</td>\n",
       "      <td>aplusk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5931c544307e01029e6614f3</td>\n",
       "      <td>@Favre4Official @TheTimMcGraw @the_USO @garthb...</td>\n",
       "      <td>2017-05-28 17:47:29</td>\n",
       "      <td>aplusk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5931c544307e01029e6614f4</td>\n",
       "      <td>RT @thorn: If we're going to win this fight, c...</td>\n",
       "      <td>2017-05-27 02:16:26</td>\n",
       "      <td>aplusk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5931c544307e01029e6614f5</td>\n",
       "      <td>RT: @dannymasterson Magic happens in Nashville...</td>\n",
       "      <td>2017-05-24 20:06:27</td>\n",
       "      <td>aplusk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5931c544307e01029e6614f6</td>\n",
       "      <td>RT @civickey: Civic is proud to introduce the ...</td>\n",
       "      <td>2017-05-23 23:54:54</td>\n",
       "      <td>aplusk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5931c544307e01029e6614f7</td>\n",
       "      <td>RT @GustoHQ: Here's why this Silicon Valley st...</td>\n",
       "      <td>2017-05-23 22:17:51</td>\n",
       "      <td>aplusk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5931c544307e01029e6614f8</td>\n",
       "      <td>RT @WarbyParker: Many thanks to @Inc for helpi...</td>\n",
       "      <td>2017-05-23 21:57:40</td>\n",
       "      <td>aplusk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5931c544307e01029e6614f9</td>\n",
       "      <td>Great to see @ResearchGate and @brkthroughpriz...</td>\n",
       "      <td>2017-05-23 17:19:07</td>\n",
       "      <td>aplusk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5931c544307e01029e6614fa</td>\n",
       "      <td>.@NFYInstitute &amp;amp; @RepKarenBass are bringin...</td>\n",
       "      <td>2017-05-22 17:49:47</td>\n",
       "      <td>aplusk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5931c544307e01029e6614fb</td>\n",
       "      <td>RT @Poshmarkapp: Know a PFF who's killin' it o...</td>\n",
       "      <td>2017-05-22 00:31:16</td>\n",
       "      <td>aplusk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5931c544307e01029e6614fc</td>\n",
       "      <td>RT @Neighborly: A great example of the value c...</td>\n",
       "      <td>2017-05-22 00:29:31</td>\n",
       "      <td>aplusk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5931c544307e01029e6614fd</td>\n",
       "      <td>@heartsandparts Hahaha</td>\n",
       "      <td>2017-05-18 00:37:53</td>\n",
       "      <td>aplusk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5931c544307e01029e6614fe</td>\n",
       "      <td>@tatadwader Pretty sure HRC's lawyers were off...</td>\n",
       "      <td>2017-05-18 00:37:24</td>\n",
       "      <td>aplusk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>5931c544307e01029e6614ff</td>\n",
       "      <td>Watching this RUSSIA probe and just thinking \"...</td>\n",
       "      <td>2017-05-18 00:34:41</td>\n",
       "      <td>aplusk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5931c544307e01029e661500</td>\n",
       "      <td>RT @Neighborly: We just raised a Series A! Mas...</td>\n",
       "      <td>2017-05-17 20:17:05</td>\n",
       "      <td>aplusk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>5931c544307e01029e661501</td>\n",
       "      <td>RT @1WorldIdentity: Congratulations to @civick...</td>\n",
       "      <td>2017-05-17 20:10:31</td>\n",
       "      <td>aplusk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5931c544307e01029e661502</td>\n",
       "      <td>we all deserve best in class cyber security. I...</td>\n",
       "      <td>2017-05-16 16:33:26</td>\n",
       "      <td>aplusk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5931c544307e01029e661503</td>\n",
       "      <td>RT @WSJ: Capsule corrals $20 million for new p...</td>\n",
       "      <td>2017-05-11 23:27:28</td>\n",
       "      <td>aplusk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5931c544307e01029e661504</td>\n",
       "      <td>.@Ludacris episode of #MyHouzz is now live, j...</td>\n",
       "      <td>2017-05-11 17:46:39</td>\n",
       "      <td>aplusk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5931c544307e01029e661505</td>\n",
       "      <td>Californias getting @lemonade_inc! Support @t...</td>\n",
       "      <td>2017-05-10 16:44:50</td>\n",
       "      <td>aplusk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>5931c544307e01029e661506</td>\n",
       "      <td>RT @moovit: We're working with university stud...</td>\n",
       "      <td>2017-05-10 02:45:16</td>\n",
       "      <td>aplusk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3169</th>\n",
       "      <td>5931c54e307e01029e66214a</td>\n",
       "      <td>who is coming to my premiere with me? http://w...</td>\n",
       "      <td>2010-12-29 17:11:53</td>\n",
       "      <td>aplusk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3170</th>\n",
       "      <td>5931c54e307e01029e66214b</td>\n",
       "      <td>@lauraeast24 got u covered kid</td>\n",
       "      <td>2010-12-29 16:44:30</td>\n",
       "      <td>aplusk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3171</th>\n",
       "      <td>5931c54e307e01029e66214c</td>\n",
       "      <td>@ulpo19 haha</td>\n",
       "      <td>2010-12-29 16:36:27</td>\n",
       "      <td>aplusk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3172</th>\n",
       "      <td>5931c54e307e01029e66214d</td>\n",
       "      <td>@ulpo19 haha</td>\n",
       "      <td>2010-12-29 16:36:27</td>\n",
       "      <td>aplusk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3173</th>\n",
       "      <td>5931c54e307e01029e66214e</td>\n",
       "      <td>@ItsTreyMorgan lost and found</td>\n",
       "      <td>2010-12-29 16:31:13</td>\n",
       "      <td>aplusk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3174</th>\n",
       "      <td>5931c54e307e01029e66214f</td>\n",
       "      <td>@127heures @Endrys no and then</td>\n",
       "      <td>2010-12-29 16:28:56</td>\n",
       "      <td>aplusk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3175</th>\n",
       "      <td>5931c54e307e01029e662150</td>\n",
       "      <td>@kelbar what like a verbert monkey?</td>\n",
       "      <td>2010-12-29 16:28:40</td>\n",
       "      <td>aplusk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3176</th>\n",
       "      <td>5931c54e307e01029e662151</td>\n",
       "      <td>@JuliReboredo she is awesome.  I can't wait fo...</td>\n",
       "      <td>2010-12-29 16:20:57</td>\n",
       "      <td>aplusk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3177</th>\n",
       "      <td>5931c54e307e01029e662152</td>\n",
       "      <td>I'm actually preparing by sipping on a bloody ...</td>\n",
       "      <td>2010-12-29 16:18:54</td>\n",
       "      <td>aplusk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3178</th>\n",
       "      <td>5931c54e307e01029e662153</td>\n",
       "      <td>@travelerspeaks no they are my words for sure....</td>\n",
       "      <td>2010-12-29 16:15:48</td>\n",
       "      <td>aplusk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3179</th>\n",
       "      <td>5931c54e307e01029e662154</td>\n",
       "      <td>@householdhacker hahaha</td>\n",
       "      <td>2010-12-29 16:09:02</td>\n",
       "      <td>aplusk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3180</th>\n",
       "      <td>5931c54e307e01029e662155</td>\n",
       "      <td>@householdhacker lol</td>\n",
       "      <td>2010-12-29 16:08:38</td>\n",
       "      <td>aplusk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3181</th>\n",
       "      <td>5931c54e307e01029e662156</td>\n",
       "      <td>Sometimes u have to do interviews just to amus...</td>\n",
       "      <td>2010-12-29 16:03:05</td>\n",
       "      <td>aplusk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3182</th>\n",
       "      <td>5931c54e307e01029e662157</td>\n",
       "      <td>for this christmas i would like to end human t...</td>\n",
       "      <td>2010-12-24 22:36:11</td>\n",
       "      <td>aplusk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3183</th>\n",
       "      <td>5931c54e307e01029e662158</td>\n",
       "      <td>nothing says christmas spirit like someone ste...</td>\n",
       "      <td>2010-12-22 14:46:23</td>\n",
       "      <td>aplusk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3184</th>\n",
       "      <td>5931c54e307e01029e662159</td>\n",
       "      <td>@mkutch i think you're ok brother</td>\n",
       "      <td>2010-12-22 14:46:02</td>\n",
       "      <td>aplusk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3185</th>\n",
       "      <td>5931c54e307e01029e66215a</td>\n",
       "      <td>@CreeperStan pretty sure it's iphone 4</td>\n",
       "      <td>2010-12-20 17:19:30</td>\n",
       "      <td>aplusk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3186</th>\n",
       "      <td>5931c54e307e01029e66215b</td>\n",
       "      <td>#win a trip 2 the premiere of my new movie 'no...</td>\n",
       "      <td>2010-12-20 15:30:05</td>\n",
       "      <td>aplusk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3187</th>\n",
       "      <td>5931c54e307e01029e66215c</td>\n",
       "      <td>@JimmyTraina Cassel is playing. I feel lucky</td>\n",
       "      <td>2010-12-19 18:12:43</td>\n",
       "      <td>aplusk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3188</th>\n",
       "      <td>5931c54e307e01029e66215d</td>\n",
       "      <td>Looking to book holiday flights try http://hip...</td>\n",
       "      <td>2010-12-18 19:13:35</td>\n",
       "      <td>aplusk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3189</th>\n",
       "      <td>5931c54e307e01029e66215e</td>\n",
       "      <td>@JimmyTraina I think u got me on the KC stl pi...</td>\n",
       "      <td>2010-12-18 14:21:24</td>\n",
       "      <td>aplusk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3190</th>\n",
       "      <td>5931c54e307e01029e66215f</td>\n",
       "      <td>I heart the movies http://t.co/HIvhWYJ via @no...</td>\n",
       "      <td>2010-12-17 17:26:40</td>\n",
       "      <td>aplusk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3191</th>\n",
       "      <td>5931c54e307e01029e662160</td>\n",
       "      <td>Rocking the new dailybooth app from Egypt. htt...</td>\n",
       "      <td>2010-12-15 17:47:18</td>\n",
       "      <td>aplusk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3192</th>\n",
       "      <td>5931c54e307e01029e662161</td>\n",
       "      <td>End human trafficking now http://post.ly/1KIxI</td>\n",
       "      <td>2010-12-12 19:27:52</td>\n",
       "      <td>aplusk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3193</th>\n",
       "      <td>5931c54e307e01029e662162</td>\n",
       "      <td>Has tech made us so in touch with each other t...</td>\n",
       "      <td>2010-12-08 21:05:30</td>\n",
       "      <td>aplusk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3194</th>\n",
       "      <td>5931c54e307e01029e662163</td>\n",
       "      <td>hope this win's music video of the year.  cool...</td>\n",
       "      <td>2010-12-07 06:01:08</td>\n",
       "      <td>aplusk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3195</th>\n",
       "      <td>5931c54e307e01029e662164</td>\n",
       "      <td>We need to end human trafficking. Donate 4 a c...</td>\n",
       "      <td>2010-12-04 18:25:36</td>\n",
       "      <td>aplusk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3196</th>\n",
       "      <td>5931c54e307e01029e662165</td>\n",
       "      <td>Clark W. Grizwald http://post.ly/1HjTn</td>\n",
       "      <td>2010-12-03 16:48:31</td>\n",
       "      <td>aplusk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3197</th>\n",
       "      <td>5931c54e307e01029e662166</td>\n",
       "      <td>you can always find someone crazy enough turn ...</td>\n",
       "      <td>2010-12-01 03:03:26</td>\n",
       "      <td>aplusk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3198</th>\n",
       "      <td>5931c54e307e01029e662167</td>\n",
       "      <td>@SamOBryant didn't mean to bomb you brother, j...</td>\n",
       "      <td>2010-11-30 01:04:20</td>\n",
       "      <td>aplusk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3199 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           _id  \\\n",
       "0     5931c544307e01029e6614e9   \n",
       "1     5931c544307e01029e6614ea   \n",
       "2     5931c544307e01029e6614eb   \n",
       "3     5931c544307e01029e6614ec   \n",
       "4     5931c544307e01029e6614ed   \n",
       "5     5931c544307e01029e6614ee   \n",
       "6     5931c544307e01029e6614ef   \n",
       "7     5931c544307e01029e6614f0   \n",
       "8     5931c544307e01029e6614f1   \n",
       "9     5931c544307e01029e6614f2   \n",
       "10    5931c544307e01029e6614f3   \n",
       "11    5931c544307e01029e6614f4   \n",
       "12    5931c544307e01029e6614f5   \n",
       "13    5931c544307e01029e6614f6   \n",
       "14    5931c544307e01029e6614f7   \n",
       "15    5931c544307e01029e6614f8   \n",
       "16    5931c544307e01029e6614f9   \n",
       "17    5931c544307e01029e6614fa   \n",
       "18    5931c544307e01029e6614fb   \n",
       "19    5931c544307e01029e6614fc   \n",
       "20    5931c544307e01029e6614fd   \n",
       "21    5931c544307e01029e6614fe   \n",
       "22    5931c544307e01029e6614ff   \n",
       "23    5931c544307e01029e661500   \n",
       "24    5931c544307e01029e661501   \n",
       "25    5931c544307e01029e661502   \n",
       "26    5931c544307e01029e661503   \n",
       "27    5931c544307e01029e661504   \n",
       "28    5931c544307e01029e661505   \n",
       "29    5931c544307e01029e661506   \n",
       "...                        ...   \n",
       "3169  5931c54e307e01029e66214a   \n",
       "3170  5931c54e307e01029e66214b   \n",
       "3171  5931c54e307e01029e66214c   \n",
       "3172  5931c54e307e01029e66214d   \n",
       "3173  5931c54e307e01029e66214e   \n",
       "3174  5931c54e307e01029e66214f   \n",
       "3175  5931c54e307e01029e662150   \n",
       "3176  5931c54e307e01029e662151   \n",
       "3177  5931c54e307e01029e662152   \n",
       "3178  5931c54e307e01029e662153   \n",
       "3179  5931c54e307e01029e662154   \n",
       "3180  5931c54e307e01029e662155   \n",
       "3181  5931c54e307e01029e662156   \n",
       "3182  5931c54e307e01029e662157   \n",
       "3183  5931c54e307e01029e662158   \n",
       "3184  5931c54e307e01029e662159   \n",
       "3185  5931c54e307e01029e66215a   \n",
       "3186  5931c54e307e01029e66215b   \n",
       "3187  5931c54e307e01029e66215c   \n",
       "3188  5931c54e307e01029e66215d   \n",
       "3189  5931c54e307e01029e66215e   \n",
       "3190  5931c54e307e01029e66215f   \n",
       "3191  5931c54e307e01029e662160   \n",
       "3192  5931c54e307e01029e662161   \n",
       "3193  5931c54e307e01029e662162   \n",
       "3194  5931c54e307e01029e662163   \n",
       "3195  5931c54e307e01029e662164   \n",
       "3196  5931c54e307e01029e662165   \n",
       "3197  5931c54e307e01029e662166   \n",
       "3198  5931c54e307e01029e662167   \n",
       "\n",
       "                                                   text           timestamp  \\\n",
       "0     RT @weatherchannel: BREAKING: Trump pulls U.S.... 2017-06-02 00:28:21   \n",
       "1     RT @elonmusk: Under Paris deal, China committe... 2017-06-02 00:12:33   \n",
       "2     RT @ddiamond: Countries that support Paris cli... 2017-06-02 00:11:58   \n",
       "3     @Billiegaga @realDonaldTrump Having a sense of... 2017-06-01 01:27:32   \n",
       "4     There has to be a new strain of medical mariju... 2017-06-01 01:23:28   \n",
       "5                 Getting ready to hit the old #Covfefe 2017-06-01 01:14:39   \n",
       "6     @garthbrooks @Favre4Official @serenawilliams @... 2017-05-30 20:54:28   \n",
       "7     @haibon_jared @thorn We are happy to have your... 2017-05-30 20:49:56   \n",
       "8     Ready to help @TheRachLindsay make some hard d... 2017-05-29 18:52:48   \n",
       "9     The right to pursue happiness belong to everyo... 2017-05-29 13:14:04   \n",
       "10    @Favre4Official @TheTimMcGraw @the_USO @garthb... 2017-05-28 17:47:29   \n",
       "11    RT @thorn: If we're going to win this fight, c... 2017-05-27 02:16:26   \n",
       "12    RT: @dannymasterson Magic happens in Nashville... 2017-05-24 20:06:27   \n",
       "13    RT @civickey: Civic is proud to introduce the ... 2017-05-23 23:54:54   \n",
       "14    RT @GustoHQ: Here's why this Silicon Valley st... 2017-05-23 22:17:51   \n",
       "15    RT @WarbyParker: Many thanks to @Inc for helpi... 2017-05-23 21:57:40   \n",
       "16    Great to see @ResearchGate and @brkthroughpriz... 2017-05-23 17:19:07   \n",
       "17    .@NFYInstitute &amp; @RepKarenBass are bringin... 2017-05-22 17:49:47   \n",
       "18    RT @Poshmarkapp: Know a PFF who's killin' it o... 2017-05-22 00:31:16   \n",
       "19    RT @Neighborly: A great example of the value c... 2017-05-22 00:29:31   \n",
       "20                               @heartsandparts Hahaha 2017-05-18 00:37:53   \n",
       "21    @tatadwader Pretty sure HRC's lawyers were off... 2017-05-18 00:37:24   \n",
       "22    Watching this RUSSIA probe and just thinking \"... 2017-05-18 00:34:41   \n",
       "23    RT @Neighborly: We just raised a Series A! Mas... 2017-05-17 20:17:05   \n",
       "24    RT @1WorldIdentity: Congratulations to @civick... 2017-05-17 20:10:31   \n",
       "25    we all deserve best in class cyber security. I... 2017-05-16 16:33:26   \n",
       "26    RT @WSJ: Capsule corrals $20 million for new p... 2017-05-11 23:27:28   \n",
       "27    .@Ludacris episode of #MyHouzz is now live, j... 2017-05-11 17:46:39   \n",
       "28    Californias getting @lemonade_inc! Support @t... 2017-05-10 16:44:50   \n",
       "29    RT @moovit: We're working with university stud... 2017-05-10 02:45:16   \n",
       "...                                                 ...                 ...   \n",
       "3169  who is coming to my premiere with me? http://w... 2010-12-29 17:11:53   \n",
       "3170                     @lauraeast24 got u covered kid 2010-12-29 16:44:30   \n",
       "3171                                       @ulpo19 haha 2010-12-29 16:36:27   \n",
       "3172                                       @ulpo19 haha 2010-12-29 16:36:27   \n",
       "3173                      @ItsTreyMorgan lost and found 2010-12-29 16:31:13   \n",
       "3174                     @127heures @Endrys no and then 2010-12-29 16:28:56   \n",
       "3175                @kelbar what like a verbert monkey? 2010-12-29 16:28:40   \n",
       "3176  @JuliReboredo she is awesome.  I can't wait fo... 2010-12-29 16:20:57   \n",
       "3177  I'm actually preparing by sipping on a bloody ... 2010-12-29 16:18:54   \n",
       "3178  @travelerspeaks no they are my words for sure.... 2010-12-29 16:15:48   \n",
       "3179                            @householdhacker hahaha 2010-12-29 16:09:02   \n",
       "3180                               @householdhacker lol 2010-12-29 16:08:38   \n",
       "3181  Sometimes u have to do interviews just to amus... 2010-12-29 16:03:05   \n",
       "3182  for this christmas i would like to end human t... 2010-12-24 22:36:11   \n",
       "3183  nothing says christmas spirit like someone ste... 2010-12-22 14:46:23   \n",
       "3184                  @mkutch i think you're ok brother 2010-12-22 14:46:02   \n",
       "3185             @CreeperStan pretty sure it's iphone 4 2010-12-20 17:19:30   \n",
       "3186  #win a trip 2 the premiere of my new movie 'no... 2010-12-20 15:30:05   \n",
       "3187       @JimmyTraina Cassel is playing. I feel lucky 2010-12-19 18:12:43   \n",
       "3188  Looking to book holiday flights try http://hip... 2010-12-18 19:13:35   \n",
       "3189  @JimmyTraina I think u got me on the KC stl pi... 2010-12-18 14:21:24   \n",
       "3190  I heart the movies http://t.co/HIvhWYJ via @no... 2010-12-17 17:26:40   \n",
       "3191  Rocking the new dailybooth app from Egypt. htt... 2010-12-15 17:47:18   \n",
       "3192     End human trafficking now http://post.ly/1KIxI 2010-12-12 19:27:52   \n",
       "3193  Has tech made us so in touch with each other t... 2010-12-08 21:05:30   \n",
       "3194  hope this win's music video of the year.  cool... 2010-12-07 06:01:08   \n",
       "3195  We need to end human trafficking. Donate 4 a c... 2010-12-04 18:25:36   \n",
       "3196             Clark W. Grizwald http://post.ly/1HjTn 2010-12-03 16:48:31   \n",
       "3197  you can always find someone crazy enough turn ... 2010-12-01 03:03:26   \n",
       "3198  @SamOBryant didn't mean to bomb you brother, j... 2010-11-30 01:04:20   \n",
       "\n",
       "     username  \n",
       "0      aplusk  \n",
       "1      aplusk  \n",
       "2      aplusk  \n",
       "3      aplusk  \n",
       "4      aplusk  \n",
       "5      aplusk  \n",
       "6      aplusk  \n",
       "7      aplusk  \n",
       "8      aplusk  \n",
       "9      aplusk  \n",
       "10     aplusk  \n",
       "11     aplusk  \n",
       "12     aplusk  \n",
       "13     aplusk  \n",
       "14     aplusk  \n",
       "15     aplusk  \n",
       "16     aplusk  \n",
       "17     aplusk  \n",
       "18     aplusk  \n",
       "19     aplusk  \n",
       "20     aplusk  \n",
       "21     aplusk  \n",
       "22     aplusk  \n",
       "23     aplusk  \n",
       "24     aplusk  \n",
       "25     aplusk  \n",
       "26     aplusk  \n",
       "27     aplusk  \n",
       "28     aplusk  \n",
       "29     aplusk  \n",
       "...       ...  \n",
       "3169   aplusk  \n",
       "3170   aplusk  \n",
       "3171   aplusk  \n",
       "3172   aplusk  \n",
       "3173   aplusk  \n",
       "3174   aplusk  \n",
       "3175   aplusk  \n",
       "3176   aplusk  \n",
       "3177   aplusk  \n",
       "3178   aplusk  \n",
       "3179   aplusk  \n",
       "3180   aplusk  \n",
       "3181   aplusk  \n",
       "3182   aplusk  \n",
       "3183   aplusk  \n",
       "3184   aplusk  \n",
       "3185   aplusk  \n",
       "3186   aplusk  \n",
       "3187   aplusk  \n",
       "3188   aplusk  \n",
       "3189   aplusk  \n",
       "3190   aplusk  \n",
       "3191   aplusk  \n",
       "3192   aplusk  \n",
       "3193   aplusk  \n",
       "3194   aplusk  \n",
       "3195   aplusk  \n",
       "3196   aplusk  \n",
       "3197   aplusk  \n",
       "3198   aplusk  \n",
       "\n",
       "[3199 rows x 4 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curs = tweets_collection.find()\n",
    "list_of_docs = []\n",
    "for i in range(curs.count()):\n",
    "    list_of_docs.append(curs.next())\n",
    "    df = pd.DataFrame(list_of_docs)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
