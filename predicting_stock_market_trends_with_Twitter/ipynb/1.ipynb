{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 -- Setup & Collection\n",
    "\n",
    "Pull Tweets from the Twitter API and collect **all Tweets from 30 tech thought leaders and news outlets**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load lib codes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/Github/predicting_stock_market_trends_with_Twitter/ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import chdir\n",
    "chdir('/home/jovyan/work/Github/predicting_stock_market_trends_with_Twitter/')\n",
    "\n",
    "from lib import *\n",
    "suppress_warnings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create database with PyMongo and install Tweepy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymongo\n",
      "  Downloading pymongo-3.4.0-cp35-cp35m-manylinux1_x86_64.whl (359kB)\n",
      "\u001b[K    100% |████████████████████████████████| 368kB 173kB/s ta 0:00:01    94% |██████████████████████████████  | 337kB 5.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tweepy\n",
      "  Downloading tweepy-3.5.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied (use --upgrade to upgrade): six>=1.7.3 in /opt/conda/lib/python3.5/site-packages (from tweepy)\n",
      "Requirement already satisfied (use --upgrade to upgrade): requests>=2.4.3 in /opt/conda/lib/python3.5/site-packages (from tweepy)\n",
      "Collecting requests-oauthlib>=0.4.1 (from tweepy)\n",
      "  Downloading requests_oauthlib-0.8.0-py2.py3-none-any.whl\n",
      "Collecting oauthlib>=0.6.2 (from requests-oauthlib>=0.4.1->tweepy)\n",
      "  Downloading oauthlib-2.0.2.tar.gz (125kB)\n",
      "\u001b[K    100% |████████████████████████████████| 133kB 3.6MB/s ta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: oauthlib\n",
      "  Running setup.py bdist_wheel for oauthlib ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jovyan/.cache/pip/wheels/84/98/7a/fba7268f61097bea6081cbe5480bc439b38975748ea7684fd5\n",
      "Successfully built oauthlib\n",
      "Installing collected packages: pymongo, oauthlib, requests-oauthlib, tweepy\n",
      "Successfully installed oauthlib-2.0.2 pymongo-3.4.0 requests-oauthlib-0.8.0 tweepy-3.5.0\n",
      "\u001b[33mYou are using pip version 8.1.2, however version 9.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pymongo tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymongo\n",
    "cli = pymongo.MongoClient(host='35.163.253.8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cli.drop_database('twitter_db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['admin', 'local']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a reference. This is not going to instantiate until you put data in it:\n",
    "twitter_db = cli.twitter_db\n",
    "cli.database_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['admin', 'local']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_collection = cli.twitter_db.twitter_collection\n",
    "cli.database_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull data from Twitter API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lib.twitter_keys import my_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Variables that contains the user credentials to access Twitter API \n",
    "access_token = my_keys['ACCESS_TOKEN']\n",
    "access_token_secret = my_keys['ACCESS_SECRET']\n",
    "consumer_key = my_keys['CONSUMER_KEY']\n",
    "consumer_secret = my_keys['CONSUMER_SECRET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Handle</th>\n",
       "      <th>Followers</th>\n",
       "      <th>Description</th>\n",
       "      <th>Type</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Carl Icahn</td>\n",
       "      <td>Carl Icahn</td>\n",
       "      <td>332,000</td>\n",
       "      <td>Chairman of Icahn Enterprises L.P.; etc., etc....</td>\n",
       "      <td>Thought Leader</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ashton Kutcher</td>\n",
       "      <td>aplusk</td>\n",
       "      <td>18,100,000</td>\n",
       "      <td>Celebrity</td>\n",
       "      <td>Celebrity</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Mark Cuban</td>\n",
       "      <td>mcuban</td>\n",
       "      <td>7,040,000</td>\n",
       "      <td>Tech investor</td>\n",
       "      <td>Celebrity</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Name      Handle    Followers   \\\n",
       "23      Carl Icahn  Carl Icahn      332,000    \n",
       "5   Ashton Kutcher      aplusk   18,100,000    \n",
       "10      Mark Cuban      mcuban    7,040,000    \n",
       "\n",
       "                                          Description            Type  \\\n",
       "23  Chairman of Icahn Enterprises L.P.; etc., etc....  Thought Leader   \n",
       "5                                           Celebrity       Celebrity   \n",
       "10                                      Tech investor       Celebrity   \n",
       "\n",
       "    Unnamed: 5  Unnamed: 6  Unnamed: 7  Unnamed: 8  \n",
       "23         NaN         NaN         NaN         NaN  \n",
       "5          NaN         NaN         NaN         NaN  \n",
       "10         NaN         NaN         NaN         NaN  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_30_df = pd.read_csv('/home/jovyan/work/Github/predicting_stock_market_trends_with_Twitter/data/twitter_users_30.csv')\n",
    "twitter_30_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "handle = list(twitter_30_df['Handle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BarackObama',\n",
       " 'cnnbrk',\n",
       " 'BillGates',\n",
       " 'realDonaldTrump',\n",
       " 'TheEconomist',\n",
       " 'aplusk',\n",
       " 'HillaryClinton',\n",
       " 'TechCrunch',\n",
       " 'elonmusk',\n",
       " 'NewYorker',\n",
       " 'mcuban',\n",
       " 'jack',\n",
       " 'MarketWatch',\n",
       " 'CNBC',\n",
       " 'ForbesTech',\n",
       " 'sacca',\n",
       " 'SAI',\n",
       " 'paulg',\n",
       " 'themotleyfool',\n",
       " 'ReformedBroker',\n",
       " 'StockTwits',\n",
       " 'cnntech',\n",
       " 'MONEY',\n",
       " 'Carl Icahn',\n",
       " 'sgblank',\n",
       " 'investorslive',\n",
       " 'markflowchatter',\n",
       " 'MarkYusko',\n",
       " 'FinancialTimes',\n",
       " 'tim_cook']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem:** Most codes on the internet use the Access Token Auth method, which is limited to 180 Requests/15 mins limit (18,000 tweets/15 mins). If you download 18K tweets before 15 mins, you won’t be able to get any more results until your 15 min window expires and you search again.\n",
    "\n",
    "**Solution:** Use Application only Auth instead of the Access Token Auth. Application only auth has higher limits - 450 request/sec (45,000 tweets/15-min), which is 2.5 times more than the Access Token Limit.\n",
    "\n",
    "The secret is the AppAuthHandler instead of the more frequent OAuthHandler which you find being used in lots of code samples. This sets up App-only Auth and gives you higher limits. Also as an added bonus notice the wait_on_rate_limit & wait_on_rate_limit_notify flags set to true. What this does is make the Tweepy API call auto wait (sleep) when it hits the rate limit and continue upon expiry of the window. This avoids you to have to program this part manually, which as you’ll shortly see makes your program much more simple and elegant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "handle = ['aplusk', 'BarackObama']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aplusk', 'BarackObama']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Replace the API_KEY and API_SECRET with your application's key and secret:\n",
    "auth = tweepy.AppAuthHandler(my_keys['CONSUMER_KEY'], my_keys['CONSUMER_SECRET'])\n",
    "\n",
    "# Authorize twitter, initialize tweepy\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "\n",
    "if (not api):\n",
    "    print (\"Can't Authenticate\")\n",
    "    sys.exit(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading max 10000000 tweets\n",
      "Downloaded 200 tweets\n",
      "Downloaded 400 tweets\n",
      "Downloaded 600 tweets\n",
      "Downloaded 799 tweets\n",
      "Downloaded 999 tweets\n",
      "Downloaded 1199 tweets\n",
      "Downloaded 1399 tweets\n",
      "Downloaded 1599 tweets\n",
      "Downloaded 1799 tweets\n",
      "Downloaded 1999 tweets\n",
      "Downloaded 2198 tweets\n",
      "Downloaded 2398 tweets\n",
      "Downloaded 2598 tweets\n",
      "Downloaded 2798 tweets\n",
      "Downloaded 2998 tweets\n",
      "Downloaded 3198 tweets\n",
      "Downloaded 3243 tweets\n",
      "No more tweets found\n",
      "\n",
      " Downloaded 3243 tweets, Saved to tweets.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "maxTweets = 10000000 # Some arbitrary large number\n",
    "tweetsPerQry = 200  # this is the max the API permits\n",
    "fName = 'tweets.csv' # We'll store the tweets in a csv file.\n",
    "\n",
    "\n",
    "# If results from a specific ID onwards are reqd, set since_id to that ID.\n",
    "# else default to no lower limit, go as far back as API allows\n",
    "sinceId = None\n",
    "\n",
    "# If results only below a specific ID are, set max_id to that ID.\n",
    "# else default to no upper limit, start from the most recent tweet matching the search query.\n",
    "max_id = -1E10\n",
    "\n",
    "tweetCount = 0\n",
    "print(\"Downloading max {0} tweets\".format(maxTweets))\n",
    "\n",
    "with open(fName, 'w') as f:\n",
    "    while tweetCount < maxTweets:\n",
    "        try:\n",
    "            if (max_id <= 0):\n",
    "                if (not sinceId):\n",
    "                    new_tweets = api.user_timeline(screen_name = 'aplusk', count=tweetsPerQry)\n",
    "                else:\n",
    "                    new_tweets = api.user_timeline(screen_name = 'aplusk', count=tweetsPerQry,\n",
    "                                            since_id=sinceId)\n",
    "            else:\n",
    "\n",
    "                if (not sinceId):\n",
    "                    new_tweets = api.user_timeline(screen_name = 'aplusk', count=tweetsPerQry,\n",
    "                                            max_id=str(max_id - 1))\n",
    "                else:\n",
    "                    new_tweets = api.user_timeline(screen_name = 'aplusk', count=tweetsPerQry,\n",
    "                                            max_id=str(max_id - 1),\n",
    "                                            since_id=sinceId)\n",
    "            if not new_tweets:\n",
    "                print(\"No more tweets found\")\n",
    "                break\n",
    "            for tweet in new_tweets:\n",
    "                outtweets = [[tweet.user.screen_name, tweet.created_at, tweet.text.encode(\"utf-8\")]]\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow([\"id\",\"created_at\",\"text\"])\n",
    "                writer.writerows(outtweets)\n",
    "              \n",
    "            tweetCount += len(new_tweets)\n",
    "            print(\"Downloaded {0} tweets\".format(tweetCount))\n",
    "            max_id = new_tweets[-1].id  \n",
    "\n",
    "        except tweepy.TweepError as e:\n",
    "            # Just exit if any error\n",
    "            print(\"some error : \" + str(e))\n",
    "            break\n",
    "\n",
    "print (\"\\n Downloaded {0} tweets, Saved to {1}\".format(tweetCount, fName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading max 10000000 tweets\n",
      "Downloaded 200 tweets\n",
      "Downloaded 400 tweets\n",
      "Downloaded 600 tweets\n",
      "Downloaded 800 tweets\n",
      "Downloaded 1000 tweets\n",
      "Downloaded 1200 tweets\n",
      "Downloaded 1400 tweets\n",
      "Downloaded 1600 tweets\n",
      "Downloaded 1800 tweets\n",
      "Downloaded 2000 tweets\n",
      "Downloaded 2200 tweets\n",
      "Downloaded 2399 tweets\n",
      "Downloaded 2599 tweets\n",
      "Downloaded 2796 tweets\n",
      "Downloaded 2996 tweets\n",
      "Downloaded 3196 tweets\n",
      "Downloaded 3233 tweets\n",
      "No more tweets found\n",
      "Downloaded 3233 tweets\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-0b20fea3b553>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mtweetCount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_tweets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Downloaded {0} tweets\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweetCount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mmax_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_tweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mtweepy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTweepError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "maxTweets = 10000000 # Some arbitrary large number\n",
    "tweetsPerQry = 200  # this is the max the API permits\n",
    "fName = 'tweets.csv' # We'll store the tweets in a csv file.\n",
    "\n",
    "\n",
    "# If results from a specific ID onwards are reqd, set since_id to that ID.\n",
    "# else default to no lower limit, go as far back as API allows\n",
    "sinceId = None\n",
    "\n",
    "# If results only below a specific ID are, set max_id to that ID.\n",
    "# else default to no upper limit, start from the most recent tweet matching the search query.\n",
    "max_id = -1E10\n",
    "\n",
    "tweetCount = 0\n",
    "print(\"Downloading max {0} tweets\".format(maxTweets))\n",
    "\n",
    "with open(fName, 'w') as f:\n",
    "    while tweetCount < maxTweets:\n",
    "        try:\n",
    "            for h in handle:\n",
    "                if (max_id <= 0):\n",
    "                    if (not sinceId):\n",
    "                        new_tweets = api.user_timeline(screen_name = h, count=tweetsPerQry)\n",
    "                    else:\n",
    "                        new_tweets = api.user_timeline(screen_name = h, count=tweetsPerQry,\n",
    "                                            since_id=sinceId)\n",
    "                else:\n",
    "\n",
    "                    if (not sinceId):\n",
    "                        new_tweets = api.user_timeline(screen_name = h, count=tweetsPerQry,\n",
    "                                            max_id=str(max_id - 1))\n",
    "                    else:\n",
    "                        new_tweets = api.user_timeline(screen_name = h, count=tweetsPerQry,\n",
    "                                            max_id=str(max_id - 1),\n",
    "                                            since_id=sinceId)\n",
    "                if not new_tweets:\n",
    "                    print(\"No more tweets found\")\n",
    "                    break\n",
    "                for tweet in new_tweets:\n",
    "                    outtweets = [[tweet.user.screen_name, tweet.created_at, tweet.text.encode(\"utf-8\")]]\n",
    "                    writer = csv.writer(f)\n",
    "                    writer.writerow([\"id\",\"created_at\",\"text\"])\n",
    "                    writer.writerows(outtweets)\n",
    "              \n",
    "            tweetCount += len(new_tweets)\n",
    "            print(\"Downloaded {0} tweets\".format(tweetCount))\n",
    "            max_id = new_tweets[-1].id  \n",
    "\n",
    "        except tweepy.TweepError as e:\n",
    "            # Just exit if any error\n",
    "            print(\"some error : \" + str(e))\n",
    "            break\n",
    "\n",
    "print (\"\\n Downloaded {0} tweets, Saved to {1}\".format(tweetCount, fName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_tweets = pd.read_csv('tweets.csv')\n",
    "all_tweets['id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!rm tweets.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
