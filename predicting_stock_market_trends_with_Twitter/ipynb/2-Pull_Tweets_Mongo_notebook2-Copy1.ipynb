{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 -- Setup & Collection \n",
    "\n",
    "Pull Tweets from the Twitter API and collect **all Tweets from 30 tech thought leaders and news outlets**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load lib codes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/Portfolio/predicting_stock_market_trends_with_Twitter\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'STOPWORDS'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-e3c3de35315e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/jovyan/work/Portfolio/predicting_stock_market_trends_with_Twitter/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# suppress_warnings()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtwitter_keys\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmy_keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'module' object has no attribute 'STOPWORDS'"
     ]
    }
   ],
   "source": [
    "from os import chdir\n",
    "chdir('/home/jovyan/work/Portfolio/predicting_stock_market_trends_with_Twitter/')\n",
    "\n",
    "from lib import *\n",
    "# suppress_warnings()\n",
    "from lib.twitter_keys import my_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pymongo pyquery tweepy\n",
    "import pymongo\n",
    "import tweepy\n",
    "# from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace the API_KEY and API_SECRET with your application's key and secret\n",
    "auth = tweepy.AppAuthHandler(my_keys['CONSUMER_KEY'], my_keys['CONSUMER_SECRET'])\n",
    "\n",
    "# Authorize twitter, initialize tweepy\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "\n",
    "if (not api):\n",
    "    print (\"Can't Authenticate\")\n",
    "    sys.exit(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#identify port for security\n",
    "cli = pymongo.MongoClient(host='52.27.11.214', port=27016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'task_collection']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiates when you put data in \n",
    "task_collection = cli.twitter_db.task_collection\n",
    "completed_collection = cli.twitter_db.completed_collection\n",
    "# tweets_collection = cli.twitter_db.tweets_collection\n",
    "cli.twitter_db.collection_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(task_collection.count())\n",
    "print(completed_collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_collection.drop()\n",
    "completed_collection.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lxml==3.5.0 (from -r requirements.txt (line 1))\n",
      "  Downloading lxml-3.5.0.tar.gz (3.8MB)\n",
      "\u001b[K    100% |████████████████████████████████| 3.8MB 147kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyquery==1.2.10 (from -r requirements.txt (line 2))\n",
      "  Downloading pyquery-1.2.10-py2-none-any.whl\n",
      "Requirement already satisfied: cssselect>0.7.9 in /opt/conda/envs/python2/lib/python2.7/site-packages (from pyquery==1.2.10->-r requirements.txt (line 2))\n",
      "Building wheels for collected packages: lxml\n",
      "  Running setup.py bdist_wheel for lxml ... \u001b[?25l-\b \b\\\b \b|\b \berror\n",
      "  Complete output from command /opt/conda/envs/python2/bin/python -u -c \"import setuptools, tokenize;__file__='/tmp/pip-build-E4xHw2/lxml/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" bdist_wheel -d /tmp/tmpRKoEDqpip-wheel- --python-tag cp27:\n",
      "  Building lxml version 3.5.0.\n",
      "  Building without Cython.\n",
      "  ERROR: /bin/sh: 1: xslt-config: not found\n",
      "  \n",
      "  ** make sure the development packages of libxml2 and libxslt are installed **\n",
      "  \n",
      "  Using build configuration of libxslt\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build/lib.linux-x86_64-2.7\n",
      "  creating build/lib.linux-x86_64-2.7/lxml\n",
      "  copying src/lxml/__init__.py -> build/lib.linux-x86_64-2.7/lxml\n",
      "  copying src/lxml/usedoctest.py -> build/lib.linux-x86_64-2.7/lxml\n",
      "  copying src/lxml/pyclasslookup.py -> build/lib.linux-x86_64-2.7/lxml\n",
      "  copying src/lxml/_elementpath.py -> build/lib.linux-x86_64-2.7/lxml\n",
      "  copying src/lxml/cssselect.py -> build/lib.linux-x86_64-2.7/lxml\n",
      "  copying src/lxml/builder.py -> build/lib.linux-x86_64-2.7/lxml\n",
      "  copying src/lxml/sax.py -> build/lib.linux-x86_64-2.7/lxml\n",
      "  copying src/lxml/doctestcompare.py -> build/lib.linux-x86_64-2.7/lxml\n",
      "  copying src/lxml/ElementInclude.py -> build/lib.linux-x86_64-2.7/lxml\n",
      "  creating build/lib.linux-x86_64-2.7/lxml/includes\n",
      "  copying src/lxml/includes/__init__.py -> build/lib.linux-x86_64-2.7/lxml/includes\n",
      "  creating build/lib.linux-x86_64-2.7/lxml/html\n",
      "  copying src/lxml/html/soupparser.py -> build/lib.linux-x86_64-2.7/lxml/html\n",
      "  copying src/lxml/html/_html5builder.py -> build/lib.linux-x86_64-2.7/lxml/html\n",
      "  copying src/lxml/html/formfill.py -> build/lib.linux-x86_64-2.7/lxml/html\n",
      "  copying src/lxml/html/__init__.py -> build/lib.linux-x86_64-2.7/lxml/html\n",
      "  copying src/lxml/html/usedoctest.py -> build/lib.linux-x86_64-2.7/lxml/html\n",
      "  copying src/lxml/html/ElementSoup.py -> build/lib.linux-x86_64-2.7/lxml/html\n",
      "  copying src/lxml/html/html5parser.py -> build/lib.linux-x86_64-2.7/lxml/html\n",
      "  copying src/lxml/html/_setmixin.py -> build/lib.linux-x86_64-2.7/lxml/html\n",
      "  copying src/lxml/html/builder.py -> build/lib.linux-x86_64-2.7/lxml/html\n",
      "  copying src/lxml/html/defs.py -> build/lib.linux-x86_64-2.7/lxml/html\n",
      "  copying src/lxml/html/_diffcommand.py -> build/lib.linux-x86_64-2.7/lxml/html\n",
      "  copying src/lxml/html/clean.py -> build/lib.linux-x86_64-2.7/lxml/html\n",
      "  copying src/lxml/html/diff.py -> build/lib.linux-x86_64-2.7/lxml/html\n",
      "  creating build/lib.linux-x86_64-2.7/lxml/isoschematron\n",
      "  copying src/lxml/isoschematron/__init__.py -> build/lib.linux-x86_64-2.7/lxml/isoschematron\n",
      "  copying src/lxml/lxml.etree.h -> build/lib.linux-x86_64-2.7/lxml\n",
      "  copying src/lxml/lxml.etree_api.h -> build/lib.linux-x86_64-2.7/lxml\n",
      "  copying src/lxml/includes/config.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n",
      "  copying src/lxml/includes/xmlerror.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n",
      "  copying src/lxml/includes/tree.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n",
      "  copying src/lxml/includes/schematron.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n",
      "  copying src/lxml/includes/htmlparser.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n",
      "  copying src/lxml/includes/xmlparser.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n",
      "  copying src/lxml/includes/xslt.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n",
      "  copying src/lxml/includes/xinclude.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n",
      "  copying src/lxml/includes/uri.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n",
      "  copying src/lxml/includes/dtdvalid.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n",
      "  copying src/lxml/includes/xpath.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n",
      "  copying src/lxml/includes/xmlschema.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n",
      "  copying src/lxml/includes/relaxng.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n",
      "  copying src/lxml/includes/etreepublic.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n",
      "  copying src/lxml/includes/c14n.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n",
      "  copying src/lxml/includes/etree_defs.h -> build/lib.linux-x86_64-2.7/lxml/includes\n",
      "  copying src/lxml/includes/lxml-version.h -> build/lib.linux-x86_64-2.7/lxml/includes\n",
      "  creating build/lib.linux-x86_64-2.7/lxml/isoschematron/resources\n",
      "  creating build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/rng\n",
      "  copying src/lxml/isoschematron/resources/rng/iso-schematron.rng -> build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/rng\n",
      "  creating build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/xsl\n",
      "  copying src/lxml/isoschematron/resources/xsl/XSD2Schtrn.xsl -> build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/xsl\n",
      "  copying src/lxml/isoschematron/resources/xsl/RNG2Schtrn.xsl -> build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/xsl\n",
      "  creating build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/xsl/iso-schematron-xslt1\n",
      "  copying src/lxml/isoschematron/resources/xsl/iso-schematron-xslt1/iso_schematron_message.xsl -> build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/xsl/iso-schematron-xslt1\n",
      "  copying src/lxml/isoschematron/resources/xsl/iso-schematron-xslt1/iso_schematron_skeleton_for_xslt1.xsl -> build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/xsl/iso-schematron-xslt1\n",
      "  copying src/lxml/isoschematron/resources/xsl/iso-schematron-xslt1/iso_dsdl_include.xsl -> build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/xsl/iso-schematron-xslt1\n",
      "  copying src/lxml/isoschematron/resources/xsl/iso-schematron-xslt1/iso_svrl_for_xslt1.xsl -> build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/xsl/iso-schematron-xslt1\n",
      "  copying src/lxml/isoschematron/resources/xsl/iso-schematron-xslt1/iso_abstract_expand.xsl -> build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/xsl/iso-schematron-xslt1\n",
      "  copying src/lxml/isoschematron/resources/xsl/iso-schematron-xslt1/readme.txt -> build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/xsl/iso-schematron-xslt1\n",
      "  running build_ext\n",
      "  building 'lxml.etree' extension\n",
      "  creating build/temp.linux-x86_64-2.7\n",
      "  creating build/temp.linux-x86_64-2.7/src\n",
      "  creating build/temp.linux-x86_64-2.7/src/lxml\n",
      "  gcc -pthread -fno-strict-aliasing -g -O2 -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -Isrc/lxml/includes -I/opt/conda/envs/python2/include/python2.7 -c src/lxml/lxml.etree.c -o build/temp.linux-x86_64-2.7/src/lxml/lxml.etree.o -w\n",
      "  In file included from src/lxml/lxml.etree.c:323:0:\n",
      "  src/lxml/includes/etree_defs.h:14:31: fatal error: libxml/xmlversion.h: No such file or directory\n",
      "   #include \"libxml/xmlversion.h\"\n",
      "                                 ^\n",
      "  compilation terminated.\n",
      "  Compile failed: command 'gcc' failed with exit status 1\n",
      "  creating tmp\n",
      "  cc -I/usr/include/libxml2 -c /tmp/xmlXPathInitHSAMSP.c -o tmp/xmlXPathInitHSAMSP.o\n",
      "  /tmp/xmlXPathInitHSAMSP.c:1:26: fatal error: libxml/xpath.h: No such file or directory\n",
      "   #include \"libxml/xpath.h\"\n",
      "                            ^\n",
      "  compilation terminated.\n",
      "  *********************************************************************************\n",
      "  Could not find function xmlCheckVersion in library libxml2. Is libxml2 installed?\n",
      "  *********************************************************************************\n",
      "  error: command 'gcc' failed with exit status 1\n",
      "  \n",
      "  ----------------------------------------\n",
      "\u001b[31m  Failed building wheel for lxml\u001b[0m\n",
      "\u001b[?25h  Running setup.py clean for lxml\n",
      "Failed to build lxml\n",
      "Installing collected packages: lxml, pyquery\n",
      "  Found existing installation: lxml 3.7.3\n",
      "    Uninstalling lxml-3.7.3:\n",
      "      Successfully uninstalled lxml-3.7.3\n",
      "  Running setup.py install for lxml ... \u001b[?25l-\b \b\\\b \b|\b \berror\n",
      "    Complete output from command /opt/conda/envs/python2/bin/python -u -c \"import setuptools, tokenize;__file__='/tmp/pip-build-E4xHw2/lxml/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" install --record /tmp/pip-ggavoX-record/install-record.txt --single-version-externally-managed --compile:\n",
      "    Building lxml version 3.5.0.\n",
      "    Building without Cython.\n",
      "    ERROR: /bin/sh: 1: xslt-config: not found\n",
      "    \n",
      "    ** make sure the development packages of libxml2 and libxslt are installed **\n",
      "    \n",
      "    Using build configuration of libxslt\n",
      "    running install\n",
      "    running build\n",
      "    running build_py\n",
      "    creating build\n",
      "    creating build/lib.linux-x86_64-2.7\n",
      "    creating build/lib.linux-x86_64-2.7/lxml\n",
      "    copying src/lxml/__init__.py -> build/lib.linux-x86_64-2.7/lxml\n",
      "    copying src/lxml/usedoctest.py -> build/lib.linux-x86_64-2.7/lxml\n",
      "    copying src/lxml/pyclasslookup.py -> build/lib.linux-x86_64-2.7/lxml\n",
      "    copying src/lxml/_elementpath.py -> build/lib.linux-x86_64-2.7/lxml\n",
      "    copying src/lxml/cssselect.py -> build/lib.linux-x86_64-2.7/lxml\n",
      "    copying src/lxml/builder.py -> build/lib.linux-x86_64-2.7/lxml\n",
      "    copying src/lxml/sax.py -> build/lib.linux-x86_64-2.7/lxml\n",
      "    copying src/lxml/doctestcompare.py -> build/lib.linux-x86_64-2.7/lxml\n",
      "    copying src/lxml/ElementInclude.py -> build/lib.linux-x86_64-2.7/lxml\n",
      "    creating build/lib.linux-x86_64-2.7/lxml/includes\n",
      "    copying src/lxml/includes/__init__.py -> build/lib.linux-x86_64-2.7/lxml/includes\n",
      "    creating build/lib.linux-x86_64-2.7/lxml/html\n",
      "    copying src/lxml/html/soupparser.py -> build/lib.linux-x86_64-2.7/lxml/html\n",
      "    copying src/lxml/html/_html5builder.py -> build/lib.linux-x86_64-2.7/lxml/html\n",
      "    copying src/lxml/html/formfill.py -> build/lib.linux-x86_64-2.7/lxml/html\n",
      "    copying src/lxml/html/__init__.py -> build/lib.linux-x86_64-2.7/lxml/html\n",
      "    copying src/lxml/html/usedoctest.py -> build/lib.linux-x86_64-2.7/lxml/html\n",
      "    copying src/lxml/html/ElementSoup.py -> build/lib.linux-x86_64-2.7/lxml/html\n",
      "    copying src/lxml/html/html5parser.py -> build/lib.linux-x86_64-2.7/lxml/html\n",
      "    copying src/lxml/html/_setmixin.py -> build/lib.linux-x86_64-2.7/lxml/html\n",
      "    copying src/lxml/html/builder.py -> build/lib.linux-x86_64-2.7/lxml/html\n",
      "    copying src/lxml/html/defs.py -> build/lib.linux-x86_64-2.7/lxml/html\n",
      "    copying src/lxml/html/_diffcommand.py -> build/lib.linux-x86_64-2.7/lxml/html\n",
      "    copying src/lxml/html/clean.py -> build/lib.linux-x86_64-2.7/lxml/html\n",
      "    copying src/lxml/html/diff.py -> build/lib.linux-x86_64-2.7/lxml/html\n",
      "    creating build/lib.linux-x86_64-2.7/lxml/isoschematron\n",
      "    copying src/lxml/isoschematron/__init__.py -> build/lib.linux-x86_64-2.7/lxml/isoschematron\n",
      "    copying src/lxml/lxml.etree.h -> build/lib.linux-x86_64-2.7/lxml\n",
      "    copying src/lxml/lxml.etree_api.h -> build/lib.linux-x86_64-2.7/lxml\n",
      "    copying src/lxml/includes/config.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n",
      "    copying src/lxml/includes/xmlerror.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n",
      "    copying src/lxml/includes/tree.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n",
      "    copying src/lxml/includes/schematron.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n",
      "    copying src/lxml/includes/htmlparser.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n",
      "    copying src/lxml/includes/xmlparser.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n",
      "    copying src/lxml/includes/xslt.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n",
      "    copying src/lxml/includes/xinclude.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n",
      "    copying src/lxml/includes/uri.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n",
      "    copying src/lxml/includes/dtdvalid.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n",
      "    copying src/lxml/includes/xpath.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n",
      "    copying src/lxml/includes/xmlschema.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n",
      "    copying src/lxml/includes/relaxng.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n",
      "    copying src/lxml/includes/etreepublic.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n",
      "    copying src/lxml/includes/c14n.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n",
      "    copying src/lxml/includes/etree_defs.h -> build/lib.linux-x86_64-2.7/lxml/includes\n",
      "    copying src/lxml/includes/lxml-version.h -> build/lib.linux-x86_64-2.7/lxml/includes\n",
      "    creating build/lib.linux-x86_64-2.7/lxml/isoschematron/resources\n",
      "    creating build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/rng\n",
      "    copying src/lxml/isoschematron/resources/rng/iso-schematron.rng -> build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/rng\n",
      "    creating build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/xsl\n",
      "    copying src/lxml/isoschematron/resources/xsl/XSD2Schtrn.xsl -> build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/xsl\n",
      "    copying src/lxml/isoschematron/resources/xsl/RNG2Schtrn.xsl -> build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/xsl\n",
      "    creating build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/xsl/iso-schematron-xslt1\n",
      "    copying src/lxml/isoschematron/resources/xsl/iso-schematron-xslt1/iso_schematron_message.xsl -> build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/xsl/iso-schematron-xslt1\n",
      "    copying src/lxml/isoschematron/resources/xsl/iso-schematron-xslt1/iso_schematron_skeleton_for_xslt1.xsl -> build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/xsl/iso-schematron-xslt1\n",
      "    copying src/lxml/isoschematron/resources/xsl/iso-schematron-xslt1/iso_dsdl_include.xsl -> build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/xsl/iso-schematron-xslt1\n",
      "    copying src/lxml/isoschematron/resources/xsl/iso-schematron-xslt1/iso_svrl_for_xslt1.xsl -> build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/xsl/iso-schematron-xslt1\n",
      "    copying src/lxml/isoschematron/resources/xsl/iso-schematron-xslt1/iso_abstract_expand.xsl -> build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/xsl/iso-schematron-xslt1\n",
      "    copying src/lxml/isoschematron/resources/xsl/iso-schematron-xslt1/readme.txt -> build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/xsl/iso-schematron-xslt1\n",
      "    running build_ext\n",
      "    building 'lxml.etree' extension\n",
      "    creating build/temp.linux-x86_64-2.7\n",
      "    creating build/temp.linux-x86_64-2.7/src\n",
      "    creating build/temp.linux-x86_64-2.7/src/lxml\n",
      "    gcc -pthread -fno-strict-aliasing -g -O2 -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -Isrc/lxml/includes -I/opt/conda/envs/python2/include/python2.7 -c src/lxml/lxml.etree.c -o build/temp.linux-x86_64-2.7/src/lxml/lxml.etree.o -w\n",
      "    In file included from src/lxml/lxml.etree.c:323:0:\n",
      "    src/lxml/includes/etree_defs.h:14:31: fatal error: libxml/xmlversion.h: No such file or directory\n",
      "     #include \"libxml/xmlversion.h\"\n",
      "                                   ^\n",
      "    compilation terminated.\n",
      "    Compile failed: command 'gcc' failed with exit status 1\n",
      "    cc -I/usr/include/libxml2 -c /tmp/xmlXPathInitcShNL2.c -o tmp/xmlXPathInitcShNL2.o\n",
      "    /tmp/xmlXPathInitcShNL2.c:1:26: fatal error: libxml/xpath.h: No such file or directory\n",
      "     #include \"libxml/xpath.h\"\n",
      "                              ^\n",
      "    compilation terminated.\n",
      "    *********************************************************************************\n",
      "    Could not find function xmlCheckVersion in library libxml2. Is libxml2 installed?\n",
      "    *********************************************************************************\n",
      "    error: command 'gcc' failed with exit status 1\n",
      "    \n",
      "    ----------------------------------------\n",
      "\u001b[?25h  Rolling back uninstall of lxml\n",
      "\u001b[31mCommand \"/opt/conda/envs/python2/bin/python -u -c \"import setuptools, tokenize;__file__='/tmp/pip-build-E4xHw2/lxml/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" install --record /tmp/pip-ggavoX-record/install-record.txt --single-version-externally-managed --compile\" failed with error code 1 in /tmp/pip-build-E4xHw2/lxml/\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "chdir('//home/jovyan/work/Portfolio/predicting_stock_market_trends_with_Twitter/tools/GetOldTweets-python-master')\n",
    "!pip install -r requirements.txt\n",
    "import got"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement distributed processing from scratch - run this on multiple AWS instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "twitter_30_df = pd.read_csv('/home/jovyan/work/Portfolio/predicting_stock_market_trends_with_Twitter/data/twitter_users_30.csv')\n",
    "handle = list(twitter_30_df['Handle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lookup_list = []\n",
    "for h in handle:\n",
    "    lookup_dict = {'handle': h,\n",
    "                   'status': 'pending'}\n",
    "    lookup_list.append(lookup_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'handle': 'BarackObama', 'status': 'pending'},\n",
       " {'handle': 'cnnbrk', 'status': 'pending'},\n",
       " {'handle': 'BillGates', 'status': 'pending'},\n",
       " {'handle': 'realDonaldTrump', 'status': 'pending'},\n",
       " {'handle': 'TheEconomist', 'status': 'pending'},\n",
       " {'handle': 'aplusk', 'status': 'pending'},\n",
       " {'handle': 'HillaryClinton', 'status': 'pending'},\n",
       " {'handle': 'TechCrunch', 'status': 'pending'},\n",
       " {'handle': 'elonmusk', 'status': 'pending'},\n",
       " {'handle': 'NewYorker', 'status': 'pending'},\n",
       " {'handle': 'mcuban', 'status': 'pending'},\n",
       " {'handle': 'jack', 'status': 'pending'},\n",
       " {'handle': 'MarketWatch', 'status': 'pending'},\n",
       " {'handle': 'CNBC', 'status': 'pending'},\n",
       " {'handle': 'ForbesTech', 'status': 'pending'},\n",
       " {'handle': 'sacca', 'status': 'pending'},\n",
       " {'handle': 'SAI', 'status': 'pending'},\n",
       " {'handle': 'paulg', 'status': 'pending'},\n",
       " {'handle': 'themotleyfool', 'status': 'pending'},\n",
       " {'handle': 'ReformedBroker', 'status': 'pending'},\n",
       " {'handle': 'StockTwits', 'status': 'pending'},\n",
       " {'handle': 'cnntech', 'status': 'pending'},\n",
       " {'handle': 'MONEY', 'status': 'pending'},\n",
       " {'handle': 'Carl_C_Icahn', 'status': 'pending'},\n",
       " {'handle': 'sgblank', 'status': 'pending'},\n",
       " {'handle': 'investorslive', 'status': 'pending'},\n",
       " {'handle': 'markflowchatter', 'status': 'pending'},\n",
       " {'handle': 'MarkYusko', 'status': 'pending'},\n",
       " {'handle': 'FinancialTimes', 'status': 'pending'},\n",
       " {'handle': 'tim_cook', 'status': 'pending'}]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookup_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dictionary in lookup_list:\n",
    "    task_collection.insert_one(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tweets_to_mongo(lookup):\n",
    "\n",
    "    maxTweets = 10000000 # Some arbitrary large number\n",
    "    tweetsPerQry = 200  # this is the max the API permits\n",
    "\n",
    "    # If results from a specific ID onwards are reqd, set since_id to that ID.\n",
    "    # else default to no lower limit, go as far back as API allows\n",
    "    sinceId = None\n",
    "\n",
    "    # If results only below a specific ID are, set max_id to that ID.\n",
    "    # else default to no upper limit, start from the most recent tweet matching the search query.\n",
    "    max_id = -1E10\n",
    "\n",
    "    tweetCount = 0\n",
    "    while tweetCount < maxTweets:\n",
    "        try:\n",
    "            if (max_id <= 0):\n",
    "                if (not sinceId):\n",
    "                    new_tweets = api.user_timeline(screen_name = lookup, count=tweetsPerQry)\n",
    "                else:\n",
    "                    new_tweets = api.user_timeline(screen_name = lookup, count=tweetsPerQry,\n",
    "                                                    since_id=sinceId)\n",
    "            else:\n",
    "\n",
    "                if (not sinceId):\n",
    "                    new_tweets = api.user_timeline(screen_name = lookup, count=tweetsPerQry,\n",
    "                                                    max_id=str(max_id - 1))\n",
    "                else:\n",
    "                    new_tweets = api.user_timeline(screen_name = lookup, count=tweetsPerQry,\n",
    "                                                    max_id=str(max_id - 1),\n",
    "                                                    since_id=sinceId)\n",
    "            if not new_tweets:\n",
    "                print(\"No more tweets found\")\n",
    "                break\n",
    "            for tweet in new_tweets:\n",
    "                tweet_dict = {'username': tweet.user.screen_name,\n",
    "                              'timestamp': tweet.created_at, \n",
    "                              'text': tweet.text.encode(\"utf-8\")}\n",
    "                mongo_response = completed_collection.insert_one(tweet_dict)\n",
    "\n",
    "            tweetCount += len(new_tweets)\n",
    "            print(\"Downloaded {0} tweets for user: {1}\".format(tweetCount, lookup))\n",
    "            max_id = new_tweets[-1].id  \n",
    "\n",
    "        except tweepy.TweepError as e:\n",
    "            # Just exit if any error\n",
    "            task_collection.insert_one({'handle': lookup, 'status': 'failed'})\n",
    "            print(\"some error : \" + str(e))\n",
    "            break\n",
    "\n",
    "    print (\"Downloaded {0} tweets for user: {1} & saved to Mongo\\n\".format(tweetCount, lookup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 200 tweets for user: BarackObama\n",
      "Downloaded 400 tweets for user: BarackObama\n",
      "Downloaded 600 tweets for user: BarackObama\n",
      "Downloaded 800 tweets for user: BarackObama\n",
      "Downloaded 1000 tweets for user: BarackObama\n",
      "Downloaded 1200 tweets for user: BarackObama\n",
      "Downloaded 1400 tweets for user: BarackObama\n",
      "Downloaded 1600 tweets for user: BarackObama\n",
      "Downloaded 1800 tweets for user: BarackObama\n",
      "Downloaded 2000 tweets for user: BarackObama\n",
      "Downloaded 2200 tweets for user: BarackObama\n",
      "Downloaded 2399 tweets for user: BarackObama\n",
      "Downloaded 2599 tweets for user: BarackObama\n",
      "Downloaded 2796 tweets for user: BarackObama\n",
      "Downloaded 2996 tweets for user: BarackObama\n",
      "Downloaded 3196 tweets for user: BarackObama\n",
      "Downloaded 3234 tweets for user: BarackObama\n",
      "No more tweets found\n",
      "Downloaded 3234 tweets for user: BarackObama & saved to Mongo\n",
      "\n",
      "Downloaded 200 tweets for user: BillGates\n",
      "Downloaded 400 tweets for user: BillGates\n",
      "Downloaded 600 tweets for user: BillGates\n",
      "Downloaded 800 tweets for user: BillGates\n",
      "Downloaded 1000 tweets for user: BillGates\n",
      "Downloaded 1200 tweets for user: BillGates\n",
      "Downloaded 1400 tweets for user: BillGates\n",
      "Downloaded 1600 tweets for user: BillGates\n",
      "Downloaded 1800 tweets for user: BillGates\n",
      "Downloaded 2000 tweets for user: BillGates\n",
      "Downloaded 2200 tweets for user: BillGates\n",
      "Downloaded 2378 tweets for user: BillGates\n",
      "No more tweets found\n",
      "Downloaded 2378 tweets for user: BillGates & saved to Mongo\n",
      "\n",
      "Downloaded 200 tweets for user: TheEconomist\n",
      "Downloaded 400 tweets for user: TheEconomist\n",
      "Downloaded 600 tweets for user: TheEconomist\n",
      "Downloaded 800 tweets for user: TheEconomist\n",
      "Downloaded 1000 tweets for user: TheEconomist\n",
      "Downloaded 1200 tweets for user: TheEconomist\n",
      "Downloaded 1400 tweets for user: TheEconomist\n",
      "Downloaded 1600 tweets for user: TheEconomist\n",
      "Downloaded 1800 tweets for user: TheEconomist\n",
      "Downloaded 2000 tweets for user: TheEconomist\n",
      "Downloaded 2200 tweets for user: TheEconomist\n",
      "Downloaded 2400 tweets for user: TheEconomist\n",
      "Downloaded 2600 tweets for user: TheEconomist\n",
      "Downloaded 2800 tweets for user: TheEconomist\n",
      "Downloaded 3000 tweets for user: TheEconomist\n",
      "Downloaded 3200 tweets for user: TheEconomist\n",
      "Downloaded 3208 tweets for user: TheEconomist\n",
      "No more tweets found\n",
      "Downloaded 3208 tweets for user: TheEconomist & saved to Mongo\n",
      "\n",
      "Downloaded 200 tweets for user: HillaryClinton\n",
      "Downloaded 400 tweets for user: HillaryClinton\n",
      "Downloaded 600 tweets for user: HillaryClinton\n",
      "Downloaded 800 tweets for user: HillaryClinton\n",
      "Downloaded 1000 tweets for user: HillaryClinton\n",
      "Downloaded 1200 tweets for user: HillaryClinton\n",
      "Downloaded 1399 tweets for user: HillaryClinton\n",
      "Downloaded 1599 tweets for user: HillaryClinton\n",
      "Downloaded 1799 tweets for user: HillaryClinton\n",
      "Downloaded 1999 tweets for user: HillaryClinton\n",
      "Downloaded 2199 tweets for user: HillaryClinton\n",
      "Downloaded 2399 tweets for user: HillaryClinton\n",
      "Downloaded 2599 tweets for user: HillaryClinton\n",
      "Downloaded 2799 tweets for user: HillaryClinton\n",
      "Downloaded 2999 tweets for user: HillaryClinton\n",
      "Downloaded 3199 tweets for user: HillaryClinton\n",
      "Downloaded 3232 tweets for user: HillaryClinton\n",
      "No more tweets found\n",
      "Downloaded 3232 tweets for user: HillaryClinton & saved to Mongo\n",
      "\n",
      "Downloaded 200 tweets for user: elonmusk\n",
      "Downloaded 400 tweets for user: elonmusk\n",
      "Downloaded 600 tweets for user: elonmusk\n",
      "Downloaded 800 tweets for user: elonmusk\n",
      "Downloaded 1000 tweets for user: elonmusk\n",
      "Downloaded 1200 tweets for user: elonmusk\n",
      "Downloaded 1399 tweets for user: elonmusk\n",
      "Downloaded 1599 tweets for user: elonmusk\n",
      "Downloaded 1799 tweets for user: elonmusk\n",
      "Downloaded 1998 tweets for user: elonmusk\n",
      "Downloaded 2198 tweets for user: elonmusk\n",
      "Downloaded 2398 tweets for user: elonmusk\n",
      "Downloaded 2598 tweets for user: elonmusk\n",
      "Downloaded 2798 tweets for user: elonmusk\n",
      "Downloaded 2997 tweets for user: elonmusk\n",
      "Downloaded 3004 tweets for user: elonmusk\n",
      "No more tweets found\n",
      "Downloaded 3004 tweets for user: elonmusk & saved to Mongo\n",
      "\n",
      "Downloaded 200 tweets for user: mcuban\n",
      "Downloaded 400 tweets for user: mcuban\n",
      "Downloaded 600 tweets for user: mcuban\n",
      "Downloaded 800 tweets for user: mcuban\n",
      "Downloaded 1000 tweets for user: mcuban\n",
      "Downloaded 1200 tweets for user: mcuban\n",
      "Downloaded 1399 tweets for user: mcuban\n",
      "Downloaded 1598 tweets for user: mcuban\n",
      "Downloaded 1797 tweets for user: mcuban\n",
      "Downloaded 1969 tweets for user: mcuban\n",
      "Downloaded 2157 tweets for user: mcuban\n",
      "No more tweets found\n",
      "Downloaded 2157 tweets for user: mcuban & saved to Mongo\n",
      "\n",
      "Downloaded 200 tweets for user: MarketWatch\n",
      "Downloaded 400 tweets for user: MarketWatch\n",
      "Downloaded 600 tweets for user: MarketWatch\n",
      "Downloaded 800 tweets for user: MarketWatch\n",
      "Downloaded 1000 tweets for user: MarketWatch\n",
      "Downloaded 1200 tweets for user: MarketWatch\n",
      "Downloaded 1400 tweets for user: MarketWatch\n",
      "Downloaded 1600 tweets for user: MarketWatch\n",
      "Downloaded 1800 tweets for user: MarketWatch\n",
      "Downloaded 2000 tweets for user: MarketWatch\n",
      "Downloaded 2200 tweets for user: MarketWatch\n",
      "Downloaded 2400 tweets for user: MarketWatch\n",
      "Downloaded 2600 tweets for user: MarketWatch\n",
      "Downloaded 2800 tweets for user: MarketWatch\n",
      "Downloaded 3000 tweets for user: MarketWatch\n",
      "Downloaded 3200 tweets for user: MarketWatch\n",
      "Downloaded 3228 tweets for user: MarketWatch\n",
      "No more tweets found\n",
      "Downloaded 3228 tweets for user: MarketWatch & saved to Mongo\n",
      "\n",
      "Downloaded 200 tweets for user: ForbesTech\n",
      "Downloaded 400 tweets for user: ForbesTech\n",
      "Downloaded 600 tweets for user: ForbesTech\n",
      "Downloaded 800 tweets for user: ForbesTech\n",
      "Downloaded 1000 tweets for user: ForbesTech\n",
      "Downloaded 1200 tweets for user: ForbesTech\n",
      "Downloaded 1400 tweets for user: ForbesTech\n",
      "Downloaded 1600 tweets for user: ForbesTech\n",
      "Downloaded 1800 tweets for user: ForbesTech\n",
      "Downloaded 2000 tweets for user: ForbesTech\n",
      "Downloaded 2200 tweets for user: ForbesTech\n",
      "Downloaded 2400 tweets for user: ForbesTech\n",
      "Downloaded 2600 tweets for user: ForbesTech\n",
      "Downloaded 2800 tweets for user: ForbesTech\n",
      "Downloaded 3000 tweets for user: ForbesTech\n",
      "Downloaded 3200 tweets for user: ForbesTech\n",
      "Downloaded 3218 tweets for user: ForbesTech\n",
      "No more tweets found\n",
      "Downloaded 3218 tweets for user: ForbesTech & saved to Mongo\n",
      "\n",
      "Downloaded 200 tweets for user: SAI\n",
      "Downloaded 400 tweets for user: SAI\n",
      "Downloaded 600 tweets for user: SAI\n",
      "Downloaded 800 tweets for user: SAI\n",
      "Downloaded 1000 tweets for user: SAI\n",
      "Downloaded 1200 tweets for user: SAI\n",
      "Downloaded 1400 tweets for user: SAI\n",
      "Downloaded 1600 tweets for user: SAI\n",
      "Downloaded 1800 tweets for user: SAI\n",
      "Downloaded 2000 tweets for user: SAI\n",
      "Downloaded 2200 tweets for user: SAI\n",
      "Downloaded 2400 tweets for user: SAI\n",
      "Downloaded 2600 tweets for user: SAI\n",
      "Downloaded 2800 tweets for user: SAI\n",
      "Downloaded 3000 tweets for user: SAI\n",
      "Downloaded 3200 tweets for user: SAI\n",
      "Downloaded 3233 tweets for user: SAI\n",
      "No more tweets found\n",
      "Downloaded 3233 tweets for user: SAI & saved to Mongo\n",
      "\n",
      "Downloaded 200 tweets for user: themotleyfool\n",
      "Downloaded 400 tweets for user: themotleyfool\n",
      "Downloaded 600 tweets for user: themotleyfool\n",
      "Downloaded 800 tweets for user: themotleyfool\n",
      "Downloaded 1000 tweets for user: themotleyfool\n",
      "Downloaded 1200 tweets for user: themotleyfool\n",
      "Downloaded 1400 tweets for user: themotleyfool\n",
      "Downloaded 1600 tweets for user: themotleyfool\n",
      "Downloaded 1800 tweets for user: themotleyfool\n",
      "Downloaded 2000 tweets for user: themotleyfool\n",
      "Downloaded 2200 tweets for user: themotleyfool\n",
      "Downloaded 2400 tweets for user: themotleyfool\n",
      "Downloaded 2600 tweets for user: themotleyfool\n",
      "Downloaded 2800 tweets for user: themotleyfool\n",
      "Downloaded 2999 tweets for user: themotleyfool\n",
      "Downloaded 3199 tweets for user: themotleyfool\n",
      "Downloaded 3225 tweets for user: themotleyfool\n",
      "No more tweets found\n",
      "Downloaded 3225 tweets for user: themotleyfool & saved to Mongo\n",
      "\n",
      "Downloaded 200 tweets for user: StockTwits\n",
      "Downloaded 400 tweets for user: StockTwits\n",
      "Downloaded 600 tweets for user: StockTwits\n",
      "Downloaded 800 tweets for user: StockTwits\n",
      "Downloaded 1000 tweets for user: StockTwits\n",
      "Downloaded 1200 tweets for user: StockTwits\n",
      "Downloaded 1400 tweets for user: StockTwits\n",
      "Downloaded 1600 tweets for user: StockTwits\n",
      "Downloaded 1800 tweets for user: StockTwits\n",
      "Downloaded 2000 tweets for user: StockTwits\n",
      "Downloaded 2200 tweets for user: StockTwits\n",
      "Downloaded 2400 tweets for user: StockTwits\n",
      "Downloaded 2600 tweets for user: StockTwits\n",
      "Downloaded 2800 tweets for user: StockTwits\n",
      "Downloaded 3000 tweets for user: StockTwits\n",
      "Downloaded 3200 tweets for user: StockTwits\n",
      "Downloaded 3222 tweets for user: StockTwits\n",
      "No more tweets found\n",
      "Downloaded 3222 tweets for user: StockTwits & saved to Mongo\n",
      "\n",
      "Downloaded 200 tweets for user: MONEY\n",
      "Downloaded 400 tweets for user: MONEY\n",
      "Downloaded 600 tweets for user: MONEY\n",
      "Downloaded 800 tweets for user: MONEY\n",
      "Downloaded 1000 tweets for user: MONEY\n",
      "Downloaded 1200 tweets for user: MONEY\n",
      "Downloaded 1400 tweets for user: MONEY\n",
      "Downloaded 1600 tweets for user: MONEY\n",
      "Downloaded 1800 tweets for user: MONEY\n",
      "Downloaded 2000 tweets for user: MONEY\n",
      "Downloaded 2200 tweets for user: MONEY\n",
      "Downloaded 2400 tweets for user: MONEY\n",
      "Downloaded 2600 tweets for user: MONEY\n",
      "Downloaded 2800 tweets for user: MONEY\n",
      "Downloaded 3000 tweets for user: MONEY\n",
      "Downloaded 3200 tweets for user: MONEY\n",
      "Downloaded 3209 tweets for user: MONEY\n",
      "No more tweets found\n",
      "Downloaded 3209 tweets for user: MONEY & saved to Mongo\n",
      "\n",
      "Downloaded 200 tweets for user: sgblank\n",
      "Downloaded 400 tweets for user: sgblank\n",
      "Downloaded 600 tweets for user: sgblank\n",
      "Downloaded 800 tweets for user: sgblank\n",
      "Downloaded 1000 tweets for user: sgblank\n",
      "Downloaded 1200 tweets for user: sgblank\n",
      "Downloaded 1400 tweets for user: sgblank\n",
      "Downloaded 1600 tweets for user: sgblank\n",
      "Downloaded 1705 tweets for user: sgblank\n",
      "No more tweets found\n",
      "Downloaded 1705 tweets for user: sgblank & saved to Mongo\n",
      "\n",
      "Downloaded 200 tweets for user: markflowchatter\n",
      "Downloaded 400 tweets for user: markflowchatter\n",
      "Downloaded 600 tweets for user: markflowchatter\n",
      "Downloaded 800 tweets for user: markflowchatter\n",
      "Downloaded 1000 tweets for user: markflowchatter\n",
      "Downloaded 1200 tweets for user: markflowchatter\n",
      "Downloaded 1400 tweets for user: markflowchatter\n",
      "Downloaded 1600 tweets for user: markflowchatter\n",
      "Downloaded 1800 tweets for user: markflowchatter\n",
      "Downloaded 2000 tweets for user: markflowchatter\n",
      "Downloaded 2200 tweets for user: markflowchatter\n",
      "Downloaded 2400 tweets for user: markflowchatter\n",
      "Downloaded 2600 tweets for user: markflowchatter\n",
      "Downloaded 2800 tweets for user: markflowchatter\n",
      "Downloaded 3000 tweets for user: markflowchatter\n",
      "Downloaded 3200 tweets for user: markflowchatter\n",
      "Downloaded 3217 tweets for user: markflowchatter\n",
      "No more tweets found\n",
      "Downloaded 3217 tweets for user: markflowchatter & saved to Mongo\n",
      "\n",
      "Downloaded 200 tweets for user: FinancialTimes\n",
      "Downloaded 400 tweets for user: FinancialTimes\n",
      "Downloaded 600 tweets for user: FinancialTimes\n",
      "Downloaded 800 tweets for user: FinancialTimes\n",
      "Downloaded 1000 tweets for user: FinancialTimes\n",
      "Downloaded 1200 tweets for user: FinancialTimes\n",
      "Downloaded 1400 tweets for user: FinancialTimes\n",
      "Downloaded 1600 tweets for user: FinancialTimes\n",
      "Downloaded 1800 tweets for user: FinancialTimes\n",
      "Downloaded 2000 tweets for user: FinancialTimes\n",
      "Downloaded 2200 tweets for user: FinancialTimes\n",
      "Downloaded 2400 tweets for user: FinancialTimes\n",
      "Downloaded 2600 tweets for user: FinancialTimes\n",
      "Downloaded 2800 tweets for user: FinancialTimes\n",
      "Downloaded 3000 tweets for user: FinancialTimes\n",
      "Downloaded 3200 tweets for user: FinancialTimes\n",
      "Downloaded 3250 tweets for user: FinancialTimes\n",
      "No more tweets found\n",
      "Downloaded 3250 tweets for user: FinancialTimes & saved to Mongo\n",
      "\n",
      "Done!\n",
      "0:02:15.607617\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "start = datetime.now()\n",
    "\n",
    "while True:\n",
    "    # Get a user from the mongo collection\n",
    "    task = task_collection.find_one_and_delete({'status': 'pending'})\n",
    "\n",
    "    if type(task) != dict:\n",
    "        print('Done!')\n",
    "        break\n",
    "\n",
    "    # Assign the handle to task_handle\n",
    "    task_handle = task['handle']\n",
    "        \n",
    "        \n",
    "    get_tweets_to_mongo(task_handle)\n",
    "    task_collection.insert_one({'handle': task['handle'], 'status': 'complete'})\n",
    "    \n",
    "end = datetime.now()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87258"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completed_collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9695333333333334"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "87258.0/90000.0\n",
    "#96.95% of possible tweets achieved - in 04:26 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9693777777777778"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "87244.0/90000.0\n",
    "#96.93% of possible tweets achieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9691333333333333"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "87222.0/90000.0\n",
    "#96.91% of possible tweets achieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>handle</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59358a7f567536001f22525e</td>\n",
       "      <td>BarackObama</td>\n",
       "      <td>complete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59358a8a567536001f225f07</td>\n",
       "      <td>cnnbrk</td>\n",
       "      <td>complete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59358a92567536001f226852</td>\n",
       "      <td>BillGates</td>\n",
       "      <td>complete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59358a9b567536001f227504</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>complete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59358aa5567536001f22818b</td>\n",
       "      <td>TheEconomist</td>\n",
       "      <td>complete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>59358aaf567536001f228e0b</td>\n",
       "      <td>aplusk</td>\n",
       "      <td>complete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>59358ab9567536001f229aac</td>\n",
       "      <td>HillaryClinton</td>\n",
       "      <td>complete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>59358ac3567536001f22a736</td>\n",
       "      <td>TechCrunch</td>\n",
       "      <td>complete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>59358acc567536001f22b2f0</td>\n",
       "      <td>elonmusk</td>\n",
       "      <td>complete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>59358ad6567536001f22bf7f</td>\n",
       "      <td>NewYorker</td>\n",
       "      <td>complete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>59358add567536001f22c7ed</td>\n",
       "      <td>mcuban</td>\n",
       "      <td>complete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>59358ae8567536001f22d469</td>\n",
       "      <td>jack</td>\n",
       "      <td>complete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>59358af2567536001f22e104</td>\n",
       "      <td>MarketWatch</td>\n",
       "      <td>complete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>59358afc567536001f22eda9</td>\n",
       "      <td>CNBC</td>\n",
       "      <td>complete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>59358b07567536001f22fa39</td>\n",
       "      <td>ForbesTech</td>\n",
       "      <td>complete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>59358b15567536001f2306d0</td>\n",
       "      <td>sacca</td>\n",
       "      <td>complete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>59358b1f567536001f23136e</td>\n",
       "      <td>SAI</td>\n",
       "      <td>complete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>59358b29567536001f231ffc</td>\n",
       "      <td>paulg</td>\n",
       "      <td>complete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>59358b33567536001f232c96</td>\n",
       "      <td>themotleyfool</td>\n",
       "      <td>complete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>59358b3e567536001f23393c</td>\n",
       "      <td>ReformedBroker</td>\n",
       "      <td>complete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>59358b48567536001f2345d3</td>\n",
       "      <td>StockTwits</td>\n",
       "      <td>complete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>59358b53567536001f235266</td>\n",
       "      <td>cnntech</td>\n",
       "      <td>complete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>59358b5c567536001f235eef</td>\n",
       "      <td>MONEY</td>\n",
       "      <td>complete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>59358b5d567536001f236031</td>\n",
       "      <td>Carl_C_Icahn</td>\n",
       "      <td>complete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>59358b62567536001f2366db</td>\n",
       "      <td>sgblank</td>\n",
       "      <td>complete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>59358b6c567536001f23735d</td>\n",
       "      <td>investorslive</td>\n",
       "      <td>complete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>59358b76567536001f237fee</td>\n",
       "      <td>markflowchatter</td>\n",
       "      <td>complete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>59358b83567536001f238c7e</td>\n",
       "      <td>MarkYusko</td>\n",
       "      <td>complete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>59358b8d567536001f23992a</td>\n",
       "      <td>FinancialTimes</td>\n",
       "      <td>complete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>59358b8e567536001f239aa5</td>\n",
       "      <td>tim_cook</td>\n",
       "      <td>complete</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         _id           handle    status\n",
       "0   59358a7f567536001f22525e      BarackObama  complete\n",
       "1   59358a8a567536001f225f07           cnnbrk  complete\n",
       "2   59358a92567536001f226852        BillGates  complete\n",
       "3   59358a9b567536001f227504  realDonaldTrump  complete\n",
       "4   59358aa5567536001f22818b     TheEconomist  complete\n",
       "5   59358aaf567536001f228e0b           aplusk  complete\n",
       "6   59358ab9567536001f229aac   HillaryClinton  complete\n",
       "7   59358ac3567536001f22a736       TechCrunch  complete\n",
       "8   59358acc567536001f22b2f0         elonmusk  complete\n",
       "9   59358ad6567536001f22bf7f        NewYorker  complete\n",
       "10  59358add567536001f22c7ed           mcuban  complete\n",
       "11  59358ae8567536001f22d469             jack  complete\n",
       "12  59358af2567536001f22e104      MarketWatch  complete\n",
       "13  59358afc567536001f22eda9             CNBC  complete\n",
       "14  59358b07567536001f22fa39       ForbesTech  complete\n",
       "15  59358b15567536001f2306d0            sacca  complete\n",
       "16  59358b1f567536001f23136e              SAI  complete\n",
       "17  59358b29567536001f231ffc            paulg  complete\n",
       "18  59358b33567536001f232c96    themotleyfool  complete\n",
       "19  59358b3e567536001f23393c   ReformedBroker  complete\n",
       "20  59358b48567536001f2345d3       StockTwits  complete\n",
       "21  59358b53567536001f235266          cnntech  complete\n",
       "22  59358b5c567536001f235eef            MONEY  complete\n",
       "23  59358b5d567536001f236031     Carl_C_Icahn  complete\n",
       "24  59358b62567536001f2366db          sgblank  complete\n",
       "25  59358b6c567536001f23735d    investorslive  complete\n",
       "26  59358b76567536001f237fee  markflowchatter  complete\n",
       "27  59358b83567536001f238c7e        MarkYusko  complete\n",
       "28  59358b8d567536001f23992a   FinancialTimes  complete\n",
       "29  59358b8e567536001f239aa5         tim_cook  complete"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curs = task_collection.find()\n",
    "list_of_docs = []\n",
    "for i in range(curs.count()):\n",
    "    list_of_docs.append(curs.next())\n",
    "    df = pd.DataFrame(list_of_docs)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>text</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>username</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5931f76f307e010746f635f4</td>\n",
       "      <td>On this National Gun Violence Awareness Day, l...</td>\n",
       "      <td>2017-06-02 17:35:54</td>\n",
       "      <td>BarackObama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5931f76f307e010746f635f5</td>\n",
       "      <td>Forever grateful for the service and sacrifice...</td>\n",
       "      <td>2017-05-29 13:09:16</td>\n",
       "      <td>BarackObama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5931f76f307e010746f635f6</td>\n",
       "      <td>Good to see my friend Prince Harry in London t...</td>\n",
       "      <td>2017-05-27 13:15:25</td>\n",
       "      <td>BarackObama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5931f76f307e010746f635f7</td>\n",
       "      <td>Through faith, love, and resolve the character...</td>\n",
       "      <td>2017-05-25 14:13:35</td>\n",
       "      <td>BarackObama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5931f76f307e010746f635f8</td>\n",
       "      <td>Our hearts go out to those killed and wounded ...</td>\n",
       "      <td>2017-05-23 16:56:14</td>\n",
       "      <td>BarackObama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5931f76f307e010746f635f9</td>\n",
       "      <td>Excited to hear from Sierra, Imani, Filiz, and...</td>\n",
       "      <td>2017-05-22 21:16:23</td>\n",
       "      <td>BarackObama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5931f76f307e010746f635fa</td>\n",
       "      <td>Happy Mother's Day to my love and partner on t...</td>\n",
       "      <td>2017-05-14 14:01:16</td>\n",
       "      <td>BarackObama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5931f76f307e010746f635fb</td>\n",
       "      <td>We're rolling up our sleeves again, back where...</td>\n",
       "      <td>2017-05-03 19:42:16</td>\n",
       "      <td>BarackObama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5931f76f307e010746f635fc</td>\n",
       "      <td>Well said, Jimmy. That's exactly why we fought...</td>\n",
       "      <td>2017-05-02 17:19:26</td>\n",
       "      <td>BarackObama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5931f76f307e010746f635fd</td>\n",
       "      <td>RT @ObamaFoundation: It’s a beautiful day on t...</td>\n",
       "      <td>2017-04-24 19:48:17</td>\n",
       "      <td>BarackObama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5931f76f307e010746f635fe</td>\n",
       "      <td>My heart goes out to the victims and their fam...</td>\n",
       "      <td>2017-03-23 13:00:20</td>\n",
       "      <td>BarackObama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5931f76f307e010746f635ff</td>\n",
       "      <td>Chuck Berry rolled over everyone who came befo...</td>\n",
       "      <td>2017-03-19 15:51:23</td>\n",
       "      <td>BarackObama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5931f76f307e010746f63600</td>\n",
       "      <td>On International Women’s Day, @MichelleObama a...</td>\n",
       "      <td>2017-03-08 21:14:58</td>\n",
       "      <td>BarackObama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5931f76f307e010746f63601</td>\n",
       "      <td>RT @ObamaFoundation: Courage comes in many for...</td>\n",
       "      <td>2017-03-02 19:00:04</td>\n",
       "      <td>BarackObama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5931f76f307e010746f63602</td>\n",
       "      <td>Humbled to be recognized by a family with a le...</td>\n",
       "      <td>2017-03-02 15:22:07</td>\n",
       "      <td>BarackObama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5931f76f307e010746f63603</td>\n",
       "      <td>We asked. You answered. https://t.co/mAJvko6VqR</td>\n",
       "      <td>2017-02-17 19:37:55</td>\n",
       "      <td>BarackObama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5931f76f307e010746f63604</td>\n",
       "      <td>Happy Valentine’s Day, @michelleobama! Almost ...</td>\n",
       "      <td>2017-02-14 15:34:47</td>\n",
       "      <td>BarackObama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5931f76f307e010746f63605</td>\n",
       "      <td>I read letters like these every single day. It...</td>\n",
       "      <td>2017-01-22 18:07:02</td>\n",
       "      <td>BarackObama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5931f76f307e010746f63606</td>\n",
       "      <td>RT @ObamaFoundation: Add your voice: https://t...</td>\n",
       "      <td>2017-01-20 21:17:01</td>\n",
       "      <td>BarackObama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5931f76f307e010746f63607</td>\n",
       "      <td>In the meantime, I want to hear what you're th...</td>\n",
       "      <td>2017-01-20 21:05:04</td>\n",
       "      <td>BarackObama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5931f76f307e010746f63608</td>\n",
       "      <td>Hi everybody! Back to the original handle. Is ...</td>\n",
       "      <td>2017-01-20 21:04:08</td>\n",
       "      <td>BarackObama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5931f76f307e010746f63609</td>\n",
       "      <td>Tonight, President Obama reflects on eight yea...</td>\n",
       "      <td>2017-01-10 23:44:14</td>\n",
       "      <td>BarackObama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>5931f76f307e010746f6360a</td>\n",
       "      <td>RT @OFA: \"Thanks to the Affordable Care Act, y...</td>\n",
       "      <td>2016-11-05 16:31:13</td>\n",
       "      <td>BarackObama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5931f76f307e010746f6360b</td>\n",
       "      <td>In the weekly address, President Obama discuss...</td>\n",
       "      <td>2016-11-05 15:38:53</td>\n",
       "      <td>BarackObama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>5931f76f307e010746f6360c</td>\n",
       "      <td>Let's keep working to keep our economy on a be...</td>\n",
       "      <td>2016-11-04 22:16:14</td>\n",
       "      <td>BarackObama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5931f76f307e010746f6360d</td>\n",
       "      <td>The landmark #ParisAgreement enters into force...</td>\n",
       "      <td>2016-11-04 18:25:16</td>\n",
       "      <td>BarackObama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5931f76f307e010746f6360e</td>\n",
       "      <td>The economy added 161,000 jobs in October, and...</td>\n",
       "      <td>2016-11-04 16:12:12</td>\n",
       "      <td>BarackObama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5931f76f307e010746f6360f</td>\n",
       "      <td>There are a lot of plans out there. Check your...</td>\n",
       "      <td>2016-11-03 21:02:36</td>\n",
       "      <td>BarackObama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5931f76f307e010746f63610</td>\n",
       "      <td>The positive impact of #Obamacare is undeniabl...</td>\n",
       "      <td>2016-11-03 19:08:49</td>\n",
       "      <td>BarackObama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>5931f76f307e010746f63611</td>\n",
       "      <td>RT @POTUS: It happened: @Cubs win World Series...</td>\n",
       "      <td>2016-11-03 17:22:00</td>\n",
       "      <td>BarackObama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5969</th>\n",
       "      <td>5931f782307e010746f64d62</td>\n",
       "      <td>Inclusion inspires innovation and communicatio...</td>\n",
       "      <td>2015-12-08 13:44:25</td>\n",
       "      <td>tim_cook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5970</th>\n",
       "      <td>5931f782307e010746f64d63</td>\n",
       "      <td>Around the world, Apple stores are going (RED)...</td>\n",
       "      <td>2015-12-01 20:49:10</td>\n",
       "      <td>tim_cook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5971</th>\n",
       "      <td>5931f782307e010746f64d64</td>\n",
       "      <td>Couldn't be prouder of @FootballAU. Great to b...</td>\n",
       "      <td>2015-11-29 15:39:33</td>\n",
       "      <td>tim_cook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5972</th>\n",
       "      <td>5931f782307e010746f64d65</td>\n",
       "      <td>Happy Thanksgiving to all! Deeply grateful thi...</td>\n",
       "      <td>2015-11-26 15:35:34</td>\n",
       "      <td>tim_cook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5973</th>\n",
       "      <td>5931f782307e010746f64d66</td>\n",
       "      <td>Prayers for Paris, the victims and their loved...</td>\n",
       "      <td>2015-11-14 00:57:22</td>\n",
       "      <td>tim_cook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5974</th>\n",
       "      <td>5931f782307e010746f64d67</td>\n",
       "      <td>Thank you Robert for being one of our very fir...</td>\n",
       "      <td>2015-11-12 20:00:42</td>\n",
       "      <td>tim_cook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5975</th>\n",
       "      <td>5931f782307e010746f64d68</td>\n",
       "      <td>Caught up with our world-class team in Cork to...</td>\n",
       "      <td>2015-11-11 22:28:28</td>\n",
       "      <td>tim_cook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5976</th>\n",
       "      <td>5931f782307e010746f64d69</td>\n",
       "      <td>Such a privilege to share this day with @tcdph...</td>\n",
       "      <td>2015-11-11 16:43:14</td>\n",
       "      <td>tim_cook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5977</th>\n",
       "      <td>5931f782307e010746f64d6a</td>\n",
       "      <td>We honor the brave military men and women who ...</td>\n",
       "      <td>2015-11-11 09:38:57</td>\n",
       "      <td>tim_cook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5978</th>\n",
       "      <td>5931f782307e010746f64d6b</td>\n",
       "      <td>Thanks @Unibocconi for the warm welcome.  In b...</td>\n",
       "      <td>2015-11-10 14:49:02</td>\n",
       "      <td>tim_cook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5979</th>\n",
       "      <td>5931f782307e010746f64d6c</td>\n",
       "      <td>Amazing what @Touchpress is doing with classic...</td>\n",
       "      <td>2015-11-09 16:09:36</td>\n",
       "      <td>tim_cook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5980</th>\n",
       "      <td>5931f782307e010746f64d6d</td>\n",
       "      <td>The Tigers are back! Congratulations @CoachGus...</td>\n",
       "      <td>2015-11-08 04:11:30</td>\n",
       "      <td>tim_cook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5981</th>\n",
       "      <td>5931f782307e010746f64d6e</td>\n",
       "      <td>Leaving the world better than we found it: 2 g...</td>\n",
       "      <td>2015-10-22 03:50:16</td>\n",
       "      <td>tim_cook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5982</th>\n",
       "      <td>5931f782307e010746f64d6f</td>\n",
       "      <td>RT @laurenepowell: Thank you @waltmossberg for...</td>\n",
       "      <td>2015-10-21 22:03:05</td>\n",
       "      <td>tim_cook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5983</th>\n",
       "      <td>5931f782307e010746f64d70</td>\n",
       "      <td>This week at Grace Hopper, we are celebrating ...</td>\n",
       "      <td>2015-10-16 13:31:16</td>\n",
       "      <td>tim_cook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5984</th>\n",
       "      <td>5931f782307e010746f64d71</td>\n",
       "      <td>Thanks to all our customers in India who queue...</td>\n",
       "      <td>2015-10-16 01:26:06</td>\n",
       "      <td>tim_cook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5985</th>\n",
       "      <td>5931f782307e010746f64d72</td>\n",
       "      <td>Prayers, condolences, and hope for all those a...</td>\n",
       "      <td>2015-10-12 01:17:19</td>\n",
       "      <td>tim_cook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5986</th>\n",
       "      <td>5931f782307e010746f64d73</td>\n",
       "      <td>A magical day for readers! Fans of #HarryPotte...</td>\n",
       "      <td>2015-10-08 13:55:34</td>\n",
       "      <td>tim_cook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5987</th>\n",
       "      <td>5931f782307e010746f64d74</td>\n",
       "      <td>Remembering Steve for who he was and what he s...</td>\n",
       "      <td>2015-10-05 11:41:49</td>\n",
       "      <td>tim_cook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5988</th>\n",
       "      <td>5931f782307e010746f64d75</td>\n",
       "      <td>Thanks to the @Newseum for an inspiring visit ...</td>\n",
       "      <td>2015-10-03 19:43:16</td>\n",
       "      <td>tim_cook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5989</th>\n",
       "      <td>5931f782307e010746f64d76</td>\n",
       "      <td>Thanks to everyone at #BoxWorks this morning! ...</td>\n",
       "      <td>2015-09-29 17:03:06</td>\n",
       "      <td>tim_cook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5990</th>\n",
       "      <td>5931f782307e010746f64d77</td>\n",
       "      <td>Thanks to our team and everyone who turned out...</td>\n",
       "      <td>2015-09-25 21:20:13</td>\n",
       "      <td>tim_cook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5991</th>\n",
       "      <td>5931f782307e010746f64d78</td>\n",
       "      <td>Thank you Vitor in Sydney! One of the very fir...</td>\n",
       "      <td>2015-09-25 01:30:25</td>\n",
       "      <td>tim_cook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5992</th>\n",
       "      <td>5931f782307e010746f64d79</td>\n",
       "      <td>RT @pschiller: Home Run!\\nPhotographer Brad Ma...</td>\n",
       "      <td>2015-09-21 20:24:25</td>\n",
       "      <td>tim_cook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5993</th>\n",
       "      <td>5931f782307e010746f64d7a</td>\n",
       "      <td>Goeiedag Brussel! Bonjour Bruxelles! http://t....</td>\n",
       "      <td>2015-09-19 16:45:19</td>\n",
       "      <td>tim_cook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5994</th>\n",
       "      <td>5931f782307e010746f64d7b</td>\n",
       "      <td>.@XQAmerica is challenging us to #RethinkHighS...</td>\n",
       "      <td>2015-09-15 17:42:29</td>\n",
       "      <td>tim_cook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5995</th>\n",
       "      <td>5931f782307e010746f64d7c</td>\n",
       "      <td>Honored to have Spencer Stone and Anthony Sadl...</td>\n",
       "      <td>2015-09-10 03:21:33</td>\n",
       "      <td>tim_cook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5996</th>\n",
       "      <td>5931f782307e010746f64d7d</td>\n",
       "      <td>It was a great day- thanks @OneRepublic, our e...</td>\n",
       "      <td>2015-09-10 01:44:00</td>\n",
       "      <td>tim_cook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5997</th>\n",
       "      <td>5931f782307e010746f64d7e</td>\n",
       "      <td>Just a few hours to go here in SF! We can’t wa...</td>\n",
       "      <td>2015-09-09 13:26:47</td>\n",
       "      <td>tim_cook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5998</th>\n",
       "      <td>5931f782307e010746f64d7f</td>\n",
       "      <td>Taking action to fight climate change is music...</td>\n",
       "      <td>2015-09-04 18:59:18</td>\n",
       "      <td>tim_cook</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5999 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           _id  \\\n",
       "0     5931f76f307e010746f635f4   \n",
       "1     5931f76f307e010746f635f5   \n",
       "2     5931f76f307e010746f635f6   \n",
       "3     5931f76f307e010746f635f7   \n",
       "4     5931f76f307e010746f635f8   \n",
       "5     5931f76f307e010746f635f9   \n",
       "6     5931f76f307e010746f635fa   \n",
       "7     5931f76f307e010746f635fb   \n",
       "8     5931f76f307e010746f635fc   \n",
       "9     5931f76f307e010746f635fd   \n",
       "10    5931f76f307e010746f635fe   \n",
       "11    5931f76f307e010746f635ff   \n",
       "12    5931f76f307e010746f63600   \n",
       "13    5931f76f307e010746f63601   \n",
       "14    5931f76f307e010746f63602   \n",
       "15    5931f76f307e010746f63603   \n",
       "16    5931f76f307e010746f63604   \n",
       "17    5931f76f307e010746f63605   \n",
       "18    5931f76f307e010746f63606   \n",
       "19    5931f76f307e010746f63607   \n",
       "20    5931f76f307e010746f63608   \n",
       "21    5931f76f307e010746f63609   \n",
       "22    5931f76f307e010746f6360a   \n",
       "23    5931f76f307e010746f6360b   \n",
       "24    5931f76f307e010746f6360c   \n",
       "25    5931f76f307e010746f6360d   \n",
       "26    5931f76f307e010746f6360e   \n",
       "27    5931f76f307e010746f6360f   \n",
       "28    5931f76f307e010746f63610   \n",
       "29    5931f76f307e010746f63611   \n",
       "...                        ...   \n",
       "5969  5931f782307e010746f64d62   \n",
       "5970  5931f782307e010746f64d63   \n",
       "5971  5931f782307e010746f64d64   \n",
       "5972  5931f782307e010746f64d65   \n",
       "5973  5931f782307e010746f64d66   \n",
       "5974  5931f782307e010746f64d67   \n",
       "5975  5931f782307e010746f64d68   \n",
       "5976  5931f782307e010746f64d69   \n",
       "5977  5931f782307e010746f64d6a   \n",
       "5978  5931f782307e010746f64d6b   \n",
       "5979  5931f782307e010746f64d6c   \n",
       "5980  5931f782307e010746f64d6d   \n",
       "5981  5931f782307e010746f64d6e   \n",
       "5982  5931f782307e010746f64d6f   \n",
       "5983  5931f782307e010746f64d70   \n",
       "5984  5931f782307e010746f64d71   \n",
       "5985  5931f782307e010746f64d72   \n",
       "5986  5931f782307e010746f64d73   \n",
       "5987  5931f782307e010746f64d74   \n",
       "5988  5931f782307e010746f64d75   \n",
       "5989  5931f782307e010746f64d76   \n",
       "5990  5931f782307e010746f64d77   \n",
       "5991  5931f782307e010746f64d78   \n",
       "5992  5931f782307e010746f64d79   \n",
       "5993  5931f782307e010746f64d7a   \n",
       "5994  5931f782307e010746f64d7b   \n",
       "5995  5931f782307e010746f64d7c   \n",
       "5996  5931f782307e010746f64d7d   \n",
       "5997  5931f782307e010746f64d7e   \n",
       "5998  5931f782307e010746f64d7f   \n",
       "\n",
       "                                                   text           timestamp  \\\n",
       "0     On this National Gun Violence Awareness Day, l... 2017-06-02 17:35:54   \n",
       "1     Forever grateful for the service and sacrifice... 2017-05-29 13:09:16   \n",
       "2     Good to see my friend Prince Harry in London t... 2017-05-27 13:15:25   \n",
       "3     Through faith, love, and resolve the character... 2017-05-25 14:13:35   \n",
       "4     Our hearts go out to those killed and wounded ... 2017-05-23 16:56:14   \n",
       "5     Excited to hear from Sierra, Imani, Filiz, and... 2017-05-22 21:16:23   \n",
       "6     Happy Mother's Day to my love and partner on t... 2017-05-14 14:01:16   \n",
       "7     We're rolling up our sleeves again, back where... 2017-05-03 19:42:16   \n",
       "8     Well said, Jimmy. That's exactly why we fought... 2017-05-02 17:19:26   \n",
       "9     RT @ObamaFoundation: It’s a beautiful day on t... 2017-04-24 19:48:17   \n",
       "10    My heart goes out to the victims and their fam... 2017-03-23 13:00:20   \n",
       "11    Chuck Berry rolled over everyone who came befo... 2017-03-19 15:51:23   \n",
       "12    On International Women’s Day, @MichelleObama a... 2017-03-08 21:14:58   \n",
       "13    RT @ObamaFoundation: Courage comes in many for... 2017-03-02 19:00:04   \n",
       "14    Humbled to be recognized by a family with a le... 2017-03-02 15:22:07   \n",
       "15      We asked. You answered. https://t.co/mAJvko6VqR 2017-02-17 19:37:55   \n",
       "16    Happy Valentine’s Day, @michelleobama! Almost ... 2017-02-14 15:34:47   \n",
       "17    I read letters like these every single day. It... 2017-01-22 18:07:02   \n",
       "18    RT @ObamaFoundation: Add your voice: https://t... 2017-01-20 21:17:01   \n",
       "19    In the meantime, I want to hear what you're th... 2017-01-20 21:05:04   \n",
       "20    Hi everybody! Back to the original handle. Is ... 2017-01-20 21:04:08   \n",
       "21    Tonight, President Obama reflects on eight yea... 2017-01-10 23:44:14   \n",
       "22    RT @OFA: \"Thanks to the Affordable Care Act, y... 2016-11-05 16:31:13   \n",
       "23    In the weekly address, President Obama discuss... 2016-11-05 15:38:53   \n",
       "24    Let's keep working to keep our economy on a be... 2016-11-04 22:16:14   \n",
       "25    The landmark #ParisAgreement enters into force... 2016-11-04 18:25:16   \n",
       "26    The economy added 161,000 jobs in October, and... 2016-11-04 16:12:12   \n",
       "27    There are a lot of plans out there. Check your... 2016-11-03 21:02:36   \n",
       "28    The positive impact of #Obamacare is undeniabl... 2016-11-03 19:08:49   \n",
       "29    RT @POTUS: It happened: @Cubs win World Series... 2016-11-03 17:22:00   \n",
       "...                                                 ...                 ...   \n",
       "5969  Inclusion inspires innovation and communicatio... 2015-12-08 13:44:25   \n",
       "5970  Around the world, Apple stores are going (RED)... 2015-12-01 20:49:10   \n",
       "5971  Couldn't be prouder of @FootballAU. Great to b... 2015-11-29 15:39:33   \n",
       "5972  Happy Thanksgiving to all! Deeply grateful thi... 2015-11-26 15:35:34   \n",
       "5973  Prayers for Paris, the victims and their loved... 2015-11-14 00:57:22   \n",
       "5974  Thank you Robert for being one of our very fir... 2015-11-12 20:00:42   \n",
       "5975  Caught up with our world-class team in Cork to... 2015-11-11 22:28:28   \n",
       "5976  Such a privilege to share this day with @tcdph... 2015-11-11 16:43:14   \n",
       "5977  We honor the brave military men and women who ... 2015-11-11 09:38:57   \n",
       "5978  Thanks @Unibocconi for the warm welcome.  In b... 2015-11-10 14:49:02   \n",
       "5979  Amazing what @Touchpress is doing with classic... 2015-11-09 16:09:36   \n",
       "5980  The Tigers are back! Congratulations @CoachGus... 2015-11-08 04:11:30   \n",
       "5981  Leaving the world better than we found it: 2 g... 2015-10-22 03:50:16   \n",
       "5982  RT @laurenepowell: Thank you @waltmossberg for... 2015-10-21 22:03:05   \n",
       "5983  This week at Grace Hopper, we are celebrating ... 2015-10-16 13:31:16   \n",
       "5984  Thanks to all our customers in India who queue... 2015-10-16 01:26:06   \n",
       "5985  Prayers, condolences, and hope for all those a... 2015-10-12 01:17:19   \n",
       "5986  A magical day for readers! Fans of #HarryPotte... 2015-10-08 13:55:34   \n",
       "5987  Remembering Steve for who he was and what he s... 2015-10-05 11:41:49   \n",
       "5988  Thanks to the @Newseum for an inspiring visit ... 2015-10-03 19:43:16   \n",
       "5989  Thanks to everyone at #BoxWorks this morning! ... 2015-09-29 17:03:06   \n",
       "5990  Thanks to our team and everyone who turned out... 2015-09-25 21:20:13   \n",
       "5991  Thank you Vitor in Sydney! One of the very fir... 2015-09-25 01:30:25   \n",
       "5992  RT @pschiller: Home Run!\\nPhotographer Brad Ma... 2015-09-21 20:24:25   \n",
       "5993  Goeiedag Brussel! Bonjour Bruxelles! http://t.... 2015-09-19 16:45:19   \n",
       "5994  .@XQAmerica is challenging us to #RethinkHighS... 2015-09-15 17:42:29   \n",
       "5995  Honored to have Spencer Stone and Anthony Sadl... 2015-09-10 03:21:33   \n",
       "5996  It was a great day- thanks @OneRepublic, our e... 2015-09-10 01:44:00   \n",
       "5997  Just a few hours to go here in SF! We can’t wa... 2015-09-09 13:26:47   \n",
       "5998  Taking action to fight climate change is music... 2015-09-04 18:59:18   \n",
       "\n",
       "         username  \n",
       "0     BarackObama  \n",
       "1     BarackObama  \n",
       "2     BarackObama  \n",
       "3     BarackObama  \n",
       "4     BarackObama  \n",
       "5     BarackObama  \n",
       "6     BarackObama  \n",
       "7     BarackObama  \n",
       "8     BarackObama  \n",
       "9     BarackObama  \n",
       "10    BarackObama  \n",
       "11    BarackObama  \n",
       "12    BarackObama  \n",
       "13    BarackObama  \n",
       "14    BarackObama  \n",
       "15    BarackObama  \n",
       "16    BarackObama  \n",
       "17    BarackObama  \n",
       "18    BarackObama  \n",
       "19    BarackObama  \n",
       "20    BarackObama  \n",
       "21    BarackObama  \n",
       "22    BarackObama  \n",
       "23    BarackObama  \n",
       "24    BarackObama  \n",
       "25    BarackObama  \n",
       "26    BarackObama  \n",
       "27    BarackObama  \n",
       "28    BarackObama  \n",
       "29    BarackObama  \n",
       "...           ...  \n",
       "5969     tim_cook  \n",
       "5970     tim_cook  \n",
       "5971     tim_cook  \n",
       "5972     tim_cook  \n",
       "5973     tim_cook  \n",
       "5974     tim_cook  \n",
       "5975     tim_cook  \n",
       "5976     tim_cook  \n",
       "5977     tim_cook  \n",
       "5978     tim_cook  \n",
       "5979     tim_cook  \n",
       "5980     tim_cook  \n",
       "5981     tim_cook  \n",
       "5982     tim_cook  \n",
       "5983     tim_cook  \n",
       "5984     tim_cook  \n",
       "5985     tim_cook  \n",
       "5986     tim_cook  \n",
       "5987     tim_cook  \n",
       "5988     tim_cook  \n",
       "5989     tim_cook  \n",
       "5990     tim_cook  \n",
       "5991     tim_cook  \n",
       "5992     tim_cook  \n",
       "5993     tim_cook  \n",
       "5994     tim_cook  \n",
       "5995     tim_cook  \n",
       "5996     tim_cook  \n",
       "5997     tim_cook  \n",
       "5998     tim_cook  \n",
       "\n",
       "[5999 rows x 4 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curs = tweets_collection.find()\n",
    "list_of_docs = []\n",
    "for i in range(curs.count()):\n",
    "    list_of_docs.append(curs.next())\n",
    "    df = pd.DataFrame(list_of_docs)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_tweets_to_mongo(lookup):\n",
    "\n",
    "#     maxTweets = 10000000 # Some arbitrary large number\n",
    "#     tweetsPerQry = 200  # this is the max the API permits\n",
    "\n",
    "#     for l in lookup:\n",
    "#         # If results from a specific ID onwards are reqd, set since_id to that ID.\n",
    "#         # else default to no lower limit, go as far back as API allows\n",
    "#         sinceId = None\n",
    "\n",
    "#         # If results only below a specific ID are, set max_id to that ID.\n",
    "#         # else default to no upper limit, start from the most recent tweet matching the search query.\n",
    "#         max_id = -1E10\n",
    "\n",
    "#         tweetCount = 0\n",
    "#         while tweetCount < maxTweets:\n",
    "#             try:\n",
    "#                 if (max_id <= 0):\n",
    "#                     if (not sinceId):\n",
    "#                         new_tweets = api.user_timeline(screen_name = 'aplusk', count=tweetsPerQry)\n",
    "#                     else:\n",
    "#                         new_tweets = api.user_timeline(screen_name = 'aplusk', count=tweetsPerQry,\n",
    "#                                                     since_id=sinceId)\n",
    "#                 else:\n",
    "\n",
    "#                     if (not sinceId):\n",
    "#                         new_tweets = api.user_timeline(screen_name = 'aplusk', count=tweetsPerQry,\n",
    "#                                                     max_id=str(max_id - 1))\n",
    "#                     else:\n",
    "#                         new_tweets = api.user_timeline(screen_name = 'aplusk', count=tweetsPerQry,\n",
    "#                                                     max_id=str(max_id - 1),\n",
    "#                                                     since_id=sinceId)\n",
    "#                 if not new_tweets:\n",
    "#                     print(\"No more tweets found\")\n",
    "#                     break\n",
    "#                 for tweet in new_tweets:\n",
    "#                     tweet_dict = {'username': tweet.user.screen_name,\n",
    "#                                   'timestamp': tweet.created_at, \n",
    "#                                   'text': tweet.text.encode(\"utf-8\")}\n",
    "#         #             writer = csv.writer(f)\n",
    "#         #             writer.writerow([\"id\",\"created_at\",\"text\"])\n",
    "#         #             writer.writerows(outtweets)\n",
    "#                     mongo_response = tweets_collection.insert_one(tweet_dict)\n",
    "\n",
    "#                 tweetCount += len(new_tweets)\n",
    "#                 print(\"Downloaded {0} tweets for user: {1}\".format(tweetCount, l))\n",
    "#                 max_id = new_tweets[-1].id  \n",
    "\n",
    "#             except tweepy.TweepError as e:\n",
    "#                 # Just exit if any error\n",
    "#                 print(\"some error : \" + str(e))\n",
    "#                 break\n",
    "\n",
    "#         print (\"Downloaded {0} tweets for user: {1} & saved to Mongo\\n\".format(tweetCount, l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "start = datetime.now()\n",
    "get_tweets_to_mongo(handle)\n",
    "end = datetime.now()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curs = tweets_collection.find()\n",
    "list_of_docs = []\n",
    "for i in range(curs.count()):\n",
    "    list_of_docs.append(curs.next())\n",
    "    df = pd.DataFrame(list_of_docs)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
